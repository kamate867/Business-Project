{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10590593,"sourceType":"datasetVersion","datasetId":6554525}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport string\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.preprocessing import StandardScaler\nimport string\nfrom concurrent.futures import ProcessPoolExecutor\nfrom time import time\nfrom numpy.lib.stride_tricks import sliding_window_view\nfrom collections import defaultdict\nfrom sklearn.model_selection import GroupShuffleSplit\nimport warnings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:44:46.894013Z","iopub.execute_input":"2025-02-03T09:44:46.894389Z","iopub.status.idle":"2025-02-03T09:44:53.994248Z","shell.execute_reply.started":"2025-02-03T09:44:46.894346Z","shell.execute_reply":"2025-02-03T09:44:53.992931Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nbase_dir = '/kaggle/input/stock-data'\n\nstockdata_list = []\nfor path in os.listdir(base_dir):\n    stockdata_list.append(os.path.join(base_dir, path))\n    stockdata_list = sorted(stockdata_list, key = lambda x: int(x.split('_')[-1].split('.')[0]))\n\nstock_df_list = []\nfor path in stockdata_list:\n    file_df = pd.read_csv(path, encoding = 'utf-8-sig', index_col = 0)\n    stock_df_list.append(file_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:44:53.995232Z","iopub.execute_input":"2025-02-03T09:44:53.995717Z","iopub.status.idle":"2025-02-03T09:45:04.990123Z","shell.execute_reply.started":"2025-02-03T09:44:53.995686Z","shell.execute_reply":"2025-02-03T09:45:04.988941Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n\ndef process_df(df):\n    final_group_list = []\n    df['일자'] = pd.to_datetime(df['일자'], format='%Y-%m-%d')\n    numeric_cols = ['시가', '고가', '저가', '현재가', '거래량']\n    for col in numeric_cols:\n        if df[col].isnull().any():\n            df[col] = df[col].fillna(0).astype(float)\n\n    grouped_dfs = [group for _, group in df.groupby('ID')]\n    for group in grouped_dfs:\n            \n        group.dropna(inplace= True)\n        group = group.sort_values(by='일자', ascending=True).copy()\n        group.reset_index(drop=True, inplace=True)\n    \n    \n        group['End Change'] = (group['현재가'] - group['현재가'].shift(1)) / group['현재가'].shift(1) * 100\n        group.dropna(subset= ['End Change'], inplace = True)\n        group.reset_index(drop=True, inplace=True)\n        condition_1 = (group['End Change'] > 29) & (group['End Change'] <= 31)\n        condition_2 = (\n                    (group['고가'].shift(-1) >= group['현재가'] * 1.29) | \n                    (group['고가'].shift(-2) >= group['현재가'] * 1.29)\n                )\n        valid_rows = condition_1 & condition_2\n        valid_indices = group.index[valid_rows]\n        start_indices = np.maximum(0, valid_indices - 35)\n        end_indices = np.minimum(valid_indices + 25, len(df))\n        \n        filtered_list = [\n            group.iloc[start:end]\n            for start, end in zip(start_indices, end_indices)\n        ]\n    \n        if filtered_list:\n            grouped_df = pd.concat(filtered_list, axis=0).drop_duplicates()\n            grouped_df = grouped_df.sort_values(by='일자', ascending=True).reset_index(drop=True)\n        else:\n            grouped_df = pd.DataFrame()  \n    \n        \n        if not grouped_df.empty:\n            grouped_df['Start Change'] = (grouped_df['시가'] - grouped_df['현재가'].shift(1)) / grouped_df['현재가'].shift(1) * 100\n            grouped_df['High Change'] = (grouped_df['고가'] - grouped_df['시가']) / grouped_df['시가'] * 100\n            grouped_df['Low Change'] = (grouped_df['저가'] - grouped_df['시가']) / grouped_df['시가'] * 100\n            grouped_df['5 Day MA'] = grouped_df['현재가'].rolling(window=5).mean()\n            grouped_df['20 Day MA'] = grouped_df['현재가'].rolling(window=20).mean()\n            grouped_df['5 Day Diff'] = np.clip( (grouped_df['5 Day MA'] - grouped_df['현재가']) / grouped_df['현재가'] * 100, -30, 30 )\n            grouped_df['20 Day Diff'] = np.clip( (grouped_df['20 Day MA'] - grouped_df['현재가']) / grouped_df['현재가'] * 100, -30, 30)\n    \n            thresholds = grouped_df['현재가'] * 1.12\n            high_array = grouped_df['고가'].to_numpy()\n            windows = sliding_window_view(high_array[1:], window_shape = 3)\n            rolling_max = np.max(windows, axis = 1) \n            rolling_max = np.concatenate([rolling_max, [np.nan] * 3])\n            grouped_df['Rolling Max'] = rolling_max\n            grouped_df['Target'] = (rolling_max >= thresholds).astype(int)\n        \n    \n            condition_1 = (grouped_df['End Change'] > 29) & (grouped_df['End Change'] <= 31)\n            condition_2 = (\n                                (grouped_df['고가'].shift(-1) >= grouped_df['현재가'] * 1.29) | \n                                (grouped_df['고가'].shift(-2) >= grouped_df['현재가'] * 1.29)\n                            )\n            valid_rows = condition_1 & condition_2\n            valid_indices = grouped_df.index[valid_rows]\n            start_indices = np.maximum(0, valid_indices - 5)\n            end_indices = np.minimum(valid_indices + 15, len(grouped_df))\n            \n            subgroup_list = [\n                grouped_df.iloc[start:end+1] \n                for start, end in zip(start_indices, end_indices)\n            ]\n        \n           \n        \n            if len(subgroup_list) >= 2:\n                i = 0\n                while i < len(subgroup_list) - 1:\n                    df_1 = subgroup_list[i]\n                    df_2 = subgroup_list[i+1]\n                    if not df_1.merge(df_2, how = 'inner').empty:\n                        new_df = pd.concat([df_1, df_2], axis = 0 ).drop_duplicates()\n                        subgroup_list[i] = new_df\n                        subgroup_list.pop(i+1)\n        \n                    else:\n                        i += 1\n            updated_subgroup_list = []\n            uppercase = list(string.ascii_uppercase)\n            lowercase = list(string.ascii_lowercase)\n            i, j = 0, 0\n            for subgroup in subgroup_list:\n                subgroup = subgroup.copy()\n                subgroup['Sub ID'] = f\"{subgroup['ID'].iloc[0]}-{uppercase[i]}-{lowercase[j]}\"\n                updated_subgroup_list.append(subgroup)\n                i += 1\n                if i == 25:\n                    i = 0\n                    j += 1\n        \n        \n            final_df = pd.concat(updated_subgroup_list, axis=0) if updated_subgroup_list else pd.DataFrame()\n            final_df = final_df[final_df['일자'] >= pd.to_datetime('20150701')]\n            final_df.reset_index(drop=True, inplace=True)\n            '''\n            subgrouped_dfs = []\n            for sub_id, group in final_df.groupby('Sub ID'): \n                group = group.copy()  \n                ss_scaler = StandardScaler()  \n                group['Trade Amount'] = ss_scaler.fit_transform(group[['거래량']]) * 10  \n                subgrouped_dfs.append(group)  \n                \n            final_df = pd.concat(subgrouped_dfs, axis=0)\n            '''\n            columns_to_round = ['End Change', 'Start Change', 'High Change', 'Low Change', '5 Day Diff', '20 Day Diff']\n            final_df.loc[:, columns_to_round] = final_df.loc[:, columns_to_round].round(2)\n            final_df.drop(columns = ['Rolling Max','시가', '고가','저가','현재가', 'ID', '5 Day MA', '20 Day MA'], inplace = True)\n            final_group_list.append(final_df)\n    return final_group_list\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:07.803563Z","iopub.execute_input":"2025-02-03T09:46:07.804037Z","iopub.status.idle":"2025-02-03T09:46:07.825633Z","shell.execute_reply.started":"2025-02-03T09:46:07.804007Z","shell.execute_reply":"2025-02-03T09:46:07.824031Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def process_df_parallel(stock_df_list):\n    with ProcessPoolExecutor() as executor:\n        processed_df_list = list(executor.map(process_df, stock_df_list))\n    return processed_df_list\n\nprocessed_df_list = process_df_parallel(stock_df_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:08.237377Z","iopub.execute_input":"2025-02-03T09:46:08.237798Z","iopub.status.idle":"2025-02-03T09:46:26.653160Z","shell.execute_reply.started":"2025-02-03T09:46:08.237765Z","shell.execute_reply":"2025-02-03T09:46:26.651654Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"temp_list = [ item for sublist in processed_df_list for item in sublist]\nscaled_df = pd.concat(temp_list, axis = 0)\nna_df = scaled_df[scaled_df['Start Change'].isna()]\ngroups_with_na = scaled_df.groupby('Sub ID').apply(lambda group: group.isna().any().any())\ngroups_with_na = groups_with_na[groups_with_na].index\ncleaned_df = scaled_df[~scaled_df['Sub ID'].isin(groups_with_na)]\nscaled_df = cleaned_df.copy()\n#test_df = scaled_df[scaled_df['일자'] >= pd.to_datetime('2024-10-01')]\n#scaled_df = scaled_df[scaled_df['일자'] < pd.to_datetime('2024-10-01')]\nscaled_size= scaled_df.groupby('Sub ID').size()\n#test_size= test_df.groupby('Sub ID').size()\nscaled_idx = scaled_size[scaled_size >= 12].index\n#test_idx = test_size[test_size >= 12].index\nscaled_df = scaled_df.loc[scaled_df['Sub ID'].isin(scaled_idx)]\n#test_df = test_df.loc[test_df['Sub ID'].isin(test_idx)]\n#scaled_df.drop(columns = ['일자', '종목'], inplace = True)\n#dropped_test_df = test_df.drop(columns = ['일자', '종목'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:41.149566Z","iopub.execute_input":"2025-02-03T09:46:41.150053Z","iopub.status.idle":"2025-02-03T09:46:42.160520Z","shell.execute_reply.started":"2025-02-03T09:46:41.150010Z","shell.execute_reply":"2025-02-03T09:46:42.159181Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-7eeb27b09383>:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  groups_with_na = scaled_df.groupby('Sub ID').apply(lambda group: group.isna().any().any())\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(len(scaled_df['Sub ID'].unique()))\nprint(scaled_df['Target'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:45.024125Z","iopub.execute_input":"2025-02-03T09:46:45.024457Z","iopub.status.idle":"2025-02-03T09:46:45.035193Z","shell.execute_reply.started":"2025-02-03T09:46:45.024432Z","shell.execute_reply":"2025-02-03T09:46:45.033961Z"}},"outputs":[{"name":"stdout","text":"1282\nTarget\n0    14828\n1    13580\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"scaled_df.to_csv('what I need.csv', encoding = 'utf-8-sig')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:47:16.906869Z","iopub.execute_input":"2025-02-03T09:47:16.907277Z","iopub.status.idle":"2025-02-03T09:47:17.165764Z","shell.execute_reply.started":"2025-02-03T09:47:16.907249Z","shell.execute_reply":"2025-02-03T09:47:17.164560Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def generate_grouped_sequences(df, group_col, target_col, min_seq_length=7):\n    sequences = []\n    grouped = df.groupby(group_col)\n\n    for sub_id, group in grouped:\n        group = group.reset_index(drop=True)\n\n        # 최소 길이가 충족되면 한 번은 반드시 실행\n        for seq_end in range(min_seq_length, len(group) + 1):  # ✅ +1 추가로 최소 길이도 포함\n            seq = group.iloc[seq_end - min_seq_length:seq_end].drop(columns=[group_col, target_col]).copy()\n\n            if '거래량' in seq.columns:\n                ss_scaler = StandardScaler()\n                seq['Trade Amount'] = ss_scaler.fit_transform(seq[['거래량']]) * 10  \n                seq['Trade Amount'] = seq['Trade Amount'].round(2)\n                seq.drop(columns=['거래량'], inplace=True)\n\n            seq = seq.to_numpy(dtype=np.float32)\n\n            target = group.iloc[seq_end - 1][target_col]  # ✅ 올바른 target 위치 지정\n\n            sequences.append((seq, target, sub_id))\n\n    return sequences\n\n\n\ndef split_data_by_group(sequences, group_col):\n\n    groups = [seq[2] for seq in sequences] \n    gss = GroupShuffleSplit(test_size = 0.2, n_splits =1, random_state = 42)\n\n    splits = []\n\n    for train_indices, val_indices in gss.split(sequences, groups = groups):\n        train_data = [sequences[i] for i in train_indices]\n        val_data = [sequences[i] for i in val_indices]\n        splits.append((train_data, val_data))\n    \n    return splits\n\n\nclass SubIDGroupedDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq, target, sub_id = self.data[idx]\n        return torch.tensor(seq, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n    targets = torch.stack(targets)\n    return padded_inputs, targets\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.3):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)  # (batch_size, seq_length, hidden_dim)\n        x = lstm_out[:, -1, :]  # 마지막 시퀀스만 사용\n        x = self.fc(x)  # 최종 출력\n        return x\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=2, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, outputs, targets):\n        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(outputs, targets)\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:31:44.247719Z","iopub.execute_input":"2025-02-03T09:31:44.247973Z","iopub.status.idle":"2025-02-03T09:31:44.263608Z","shell.execute_reply.started":"2025-02-03T09:31:44.247942Z","shell.execute_reply":"2025-02-03T09:31:44.262703Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"sequences = generate_grouped_sequences(\n    scaled_df, group_col='Sub ID', target_col='Target', min_seq_length=7\n)\nsplits = split_data_by_group(sequences, group_col = 'Sub ID')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:31:44.264524Z","iopub.execute_input":"2025-02-03T09:31:44.264820Z","iopub.status.idle":"2025-02-03T09:32:50.755132Z","shell.execute_reply.started":"2025-02-03T09:31:44.264792Z","shell.execute_reply":"2025-02-03T09:32:50.754397Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"train_data, val_data = splits[0]\ntrain_dataset = SubIDGroupedDataset(train_data)\nval_dataset = SubIDGroupedDataset(val_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\ninput_dim = scaled_df.shape[1] - 2 \nhidden_dim = 512\nnum_layers = 5\noutput_dim = 1  # 이진 분류\n\nmodel = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout=0)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = FocalLoss() \noptimizer = optim.Adam(model.parameters(), lr=1e-4 )\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 15\nmodel_save_dir = \"saved_models_1\"\nos.makedirs(model_save_dir, exist_ok=True)  # 모델 저장 폴더 생성\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        targets = targets.float()\n\n        optimizer.zero_grad()\n        outputs = model(inputs).squeeze(1)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n    # 검증\n    model.eval()\n    total_val_loss = 0\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs).squeeze(1)\n            loss = criterion(outputs, targets)\n            total_val_loss += loss.item()\n            preds = (torch.sigmoid(outputs) > 0.65).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(targets.cpu().numpy())\n\n    precision = precision_score(all_targets, all_preds, zero_division=1)\n    recall = recall_score(all_targets, all_preds, zero_division=1)\n    scheduler.step(total_val_loss)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, \"\n          f\"Val Loss: {total_val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n    # 모델 가중치 저장\n    model_save_path = os.path.join(model_save_dir, f\"model_epoch_{epoch+1}.pth\")\n    torch.save(model.state_dict(), model_save_path)\n\nprint(\"\\n===== 모든 에포크 모델 저장 완료 =====\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:32:50.757472Z","iopub.execute_input":"2025-02-03T09:32:50.757738Z","iopub.status.idle":"2025-02-03T09:34:06.598285Z","shell.execute_reply.started":"2025-02-03T09:32:50.757715Z","shell.execute_reply":"2025-02-03T09:34:06.597234Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Train Loss: 156.5791, Val Loss: 38.3304, Precision: 1.0000, Recall: 0.0000\nEpoch 2/15, Train Loss: 151.1154, Val Loss: 37.6584, Precision: 0.8947, Recall: 0.0120\nEpoch 3/15, Train Loss: 149.7340, Val Loss: 37.4354, Precision: 0.8791, Recall: 0.0565\nEpoch 4/15, Train Loss: 148.7586, Val Loss: 37.3810, Precision: 1.0000, Recall: 0.0014\nEpoch 5/15, Train Loss: 148.3853, Val Loss: 37.5712, Precision: 0.9000, Recall: 0.0381\nEpoch 6/15, Train Loss: 147.4862, Val Loss: 37.8461, Precision: 0.8788, Recall: 0.0614\nEpoch 7/15, Train Loss: 146.7066, Val Loss: 37.6591, Precision: 0.8519, Recall: 0.0974\nEpoch 8/15, Train Loss: 144.9141, Val Loss: 37.4829, Precision: 0.8692, Recall: 0.0797\nEpoch 9/15, Train Loss: 143.7939, Val Loss: 37.5530, Precision: 0.8661, Recall: 0.0776\nEpoch 10/15, Train Loss: 142.8494, Val Loss: 38.1004, Precision: 0.8509, Recall: 0.0685\nEpoch 11/15, Train Loss: 140.6478, Val Loss: 38.6033, Precision: 0.7460, Recall: 0.0995\nEpoch 12/15, Train Loss: 139.4359, Val Loss: 38.6358, Precision: 0.8480, Recall: 0.0748\nEpoch 13/15, Train Loss: 138.4612, Val Loss: 39.0380, Precision: 0.7604, Recall: 0.1164\nEpoch 14/15, Train Loss: 136.7338, Val Loss: 39.8541, Precision: 0.7264, Recall: 0.1087\nEpoch 15/15, Train Loss: 135.8474, Val Loss: 39.9484, Precision: 0.7206, Recall: 0.1037\n\n===== 모든 에포크 모델 저장 완료 =====\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"train_data, val_data = splits[0]\ntrain_dataset = SubIDGroupedDataset(train_data)\nval_dataset = SubIDGroupedDataset(val_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n\ninput_dim = scaled_df.shape[1] - 2 \nhidden_dim =128\nnum_layers =4\noutput_dim = 1  # 이진 분류\n\nmodel = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout=0.2)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = FocalLoss() \noptimizer = optim.Adam(model.parameters(), lr=1e-4 *2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 15\nmodel_save_dir = \"saved_models_2\"\nos.makedirs(model_save_dir, exist_ok=True)  # 모델 저장 폴더 생성\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        targets = targets.float()\n\n        optimizer.zero_grad()\n        outputs = model(inputs).squeeze(1)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n    # 검증\n    model.eval()\n    total_val_loss = 0\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs).squeeze(1)\n            loss = criterion(outputs, targets)\n            total_val_loss += loss.item()\n            preds = (torch.sigmoid(outputs) > 0.65).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(targets.cpu().numpy())\n\n    precision = precision_score(all_targets, all_preds, zero_division=1)\n    recall = recall_score(all_targets, all_preds, zero_division=1)\n    scheduler.step(total_val_loss)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, \"\n          f\"Val Loss: {total_val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n    # 모델 가중치 저장\n    model_save_path = os.path.join(model_save_dir, f\"model_epoch_{epoch+1}.pth\")\n    torch.save(model.state_dict(), model_save_path)\n\nprint(\"\\n===== 모든 에포크 모델 저장 완료 =====\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:34:06.599432Z","iopub.execute_input":"2025-02-03T09:34:06.599652Z","iopub.status.idle":"2025-02-03T09:35:10.083648Z","shell.execute_reply.started":"2025-02-03T09:34:06.599634Z","shell.execute_reply":"2025-02-03T09:35:10.082724Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Train Loss: 309.9163, Val Loss: 75.2983, Precision: 0.8302, Recall: 0.0311\nEpoch 2/15, Train Loss: 300.7168, Val Loss: 75.2464, Precision: 0.8846, Recall: 0.0487\nEpoch 3/15, Train Loss: 298.7589, Val Loss: 74.8806, Precision: 0.8679, Recall: 0.0325\nEpoch 4/15, Train Loss: 296.6977, Val Loss: 74.3986, Precision: 0.9348, Recall: 0.0303\nEpoch 5/15, Train Loss: 294.7629, Val Loss: 75.4754, Precision: 0.8417, Recall: 0.0713\nEpoch 6/15, Train Loss: 293.0445, Val Loss: 75.0407, Precision: 0.8852, Recall: 0.0381\nEpoch 7/15, Train Loss: 290.5242, Val Loss: 75.8473, Precision: 0.8750, Recall: 0.0148\nEpoch 8/15, Train Loss: 286.9823, Val Loss: 76.0934, Precision: 0.7698, Recall: 0.0755\nEpoch 9/15, Train Loss: 285.0146, Val Loss: 76.5017, Precision: 0.8182, Recall: 0.0572\nEpoch 10/15, Train Loss: 283.4439, Val Loss: 76.5853, Precision: 0.7610, Recall: 0.0854\nEpoch 11/15, Train Loss: 280.5492, Val Loss: 76.7328, Precision: 0.8049, Recall: 0.0699\nEpoch 12/15, Train Loss: 278.2431, Val Loss: 78.0916, Precision: 0.7636, Recall: 0.0593\nEpoch 13/15, Train Loss: 277.6566, Val Loss: 77.7588, Precision: 0.7730, Recall: 0.0769\nEpoch 14/15, Train Loss: 276.0939, Val Loss: 77.7615, Precision: 0.7937, Recall: 0.0706\nEpoch 15/15, Train Loss: 275.4985, Val Loss: 78.4566, Precision: 0.7548, Recall: 0.0826\n\n===== 모든 에포크 모델 저장 완료 =====\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"scaled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:36:45.486579Z","iopub.execute_input":"2025-02-03T09:36:45.486925Z","iopub.status.idle":"2025-02-03T09:36:45.504262Z","shell.execute_reply.started":"2025-02-03T09:36:45.486899Z","shell.execute_reply":"2025-02-03T09:36:45.503278Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"          거래량  End Change  Start Change  High Change  Low Change  5 Day Diff  \\\n0    356243.0       -2.94         -0.32         1.37       -2.95        6.80   \n1    215563.0        0.76          0.00         2.71       -0.33        3.42   \n2   2638761.0       10.11          0.00        15.70        0.00       -6.07   \n3    672884.0       -0.98          1.56         2.88       -5.77       -4.50   \n4    255844.0       -2.96         -0.99         1.39       -2.39       -0.91   \n..        ...         ...           ...          ...         ...         ...   \n16    39306.0        0.48         -0.94         2.39       -1.91        1.70   \n17    33877.0        2.83         -0.47         4.27       -1.42       -1.28   \n18    25727.0       -2.29          0.46         0.00       -4.11        0.28   \n19    53288.0       -1.88          0.00         0.46       -4.23        1.72   \n20    40788.0       -1.91         -0.48         1.92       -1.92        3.12   \n\n    20 Day Diff  Target      Sub ID  \n0         23.85       1  373110-A-a  \n1         20.78       1  373110-A-a  \n2          8.25       1  373110-A-a  \n3          7.63       1  373110-A-a  \n4          9.35       1  373110-A-a  \n..          ...     ...         ...  \n16        11.70       0    2420-A-a  \n17         7.96       0    2420-A-a  \n18         9.69       0    2420-A-a  \n19        10.88       0    2420-A-a  \n20        12.02       0    2420-A-a  \n\n[26457 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>거래량</th>\n      <th>End Change</th>\n      <th>Start Change</th>\n      <th>High Change</th>\n      <th>Low Change</th>\n      <th>5 Day Diff</th>\n      <th>20 Day Diff</th>\n      <th>Target</th>\n      <th>Sub ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>356243.0</td>\n      <td>-2.94</td>\n      <td>-0.32</td>\n      <td>1.37</td>\n      <td>-2.95</td>\n      <td>6.80</td>\n      <td>23.85</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>215563.0</td>\n      <td>0.76</td>\n      <td>0.00</td>\n      <td>2.71</td>\n      <td>-0.33</td>\n      <td>3.42</td>\n      <td>20.78</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2638761.0</td>\n      <td>10.11</td>\n      <td>0.00</td>\n      <td>15.70</td>\n      <td>0.00</td>\n      <td>-6.07</td>\n      <td>8.25</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>672884.0</td>\n      <td>-0.98</td>\n      <td>1.56</td>\n      <td>2.88</td>\n      <td>-5.77</td>\n      <td>-4.50</td>\n      <td>7.63</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>255844.0</td>\n      <td>-2.96</td>\n      <td>-0.99</td>\n      <td>1.39</td>\n      <td>-2.39</td>\n      <td>-0.91</td>\n      <td>9.35</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>39306.0</td>\n      <td>0.48</td>\n      <td>-0.94</td>\n      <td>2.39</td>\n      <td>-1.91</td>\n      <td>1.70</td>\n      <td>11.70</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>33877.0</td>\n      <td>2.83</td>\n      <td>-0.47</td>\n      <td>4.27</td>\n      <td>-1.42</td>\n      <td>-1.28</td>\n      <td>7.96</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>25727.0</td>\n      <td>-2.29</td>\n      <td>0.46</td>\n      <td>0.00</td>\n      <td>-4.11</td>\n      <td>0.28</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>53288.0</td>\n      <td>-1.88</td>\n      <td>0.00</td>\n      <td>0.46</td>\n      <td>-4.23</td>\n      <td>1.72</td>\n      <td>10.88</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>40788.0</td>\n      <td>-1.91</td>\n      <td>-0.48</td>\n      <td>1.92</td>\n      <td>-1.92</td>\n      <td>3.12</td>\n      <td>12.02</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n  </tbody>\n</table>\n<p>26457 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"dropped_test_df.reset_index(drop = True, inplace = True)\ndef generate_grouped_sequences(df, group_col, target_col, min_seq_length=7):\n    sequences = []\n    indices = []  # ✅ 예측할 데이터의 원본 인덱스 저장\n    \n    df = df.copy()  # 원본 DataFrame 유지\n    df[\"original_index\"] = df.index  # ✅ 원래 인덱스를 새로운 열로 저장\n    \n    grouped = df.groupby(group_col)\n\n    for sub_id, group in grouped:\n        group = group.reset_index(drop=True)  # ✅ 여기서 reset_index() 하면 original_index는 유지됨\n        \n        if len(group) >= min_seq_length:\n            for seq_end in range(min_seq_length, len(group) + 1):  \n                # ✅ 필요 없는 컬럼 제외하고, 정확하게 7개의 Feature만 사용\n                if '거래량' in group.columns:\n                    ss_scaler = StandardScaler()\n                    group['Trade Amount'] = ss_scaler.fit_transform(group[['거래량']]) * 10  \n                    group['Trade Amount'] = group['Trade Amount'].round(2)\n\n                feature_cols = ['End Change', 'Start Change', 'High Change', 'Low Change', '5 Day Diff', '20 Day Diff', 'Trade Amount']\n                seq = group.iloc[seq_end - min_seq_length:seq_end][feature_cols].copy()\n                seq = seq.to_numpy(dtype=np.float32)\n\n                target = group.iloc[seq_end - 1][target_col]  \n                original_index = group.iloc[seq_end - 1]['original_index']  # ✅ 원래 dropped_test_df의 인덱스를 가져옴\n\n                sequences.append((seq, target, sub_id))\n                indices.append(original_index)\n\n    # ✅ input_dim을 강제적으로 7로 맞추었는지 확인\n    input_dim = sequences[0][0].shape[-1] if sequences else 0\n    print(f\"🔹 최종 입력 데이터 차원 (input_dim): {input_dim}\")\n\n    return sequences, indices, input_dim\n\n\n\n\n# ✅ 2. 테스트 데이터셋 준비 (수정된 `generate_grouped_sequences` 적용)\nsequences, indices, input_dim = generate_grouped_sequences(dropped_test_df, group_col='Sub ID', target_col='Target', min_seq_length=7)\ntest_dataset = SubIDGroupedDataset(sequences)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n\ninput_dim = 7  # ✅ 저장된 모델과 일치하도록 변경\nhidden_dim =128\nnum_layers =4 #✅ 저장된 모델과 일치하도록 변경\noutput_dim = 1  \n\nmodel = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout=0.2).to(device)\n\n# ✅ 4. 모델 가중치 불러오기\nmodel_path = \"/kaggle/working/saved_models_2/model_epoch_9.pth\"\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\n# ✅ 5. 모델 평가 (예측 확률 포함)\nall_preds, all_targets, all_probs = [], [], []\nvalidation_results = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs).squeeze(1)\n\n        probs = torch.sigmoid(outputs).cpu().numpy()  # 확률 계산\n        preds = (probs > 0.65).astype(int)  # 예측 결과\n\n        all_preds.extend(preds)\n        all_probs.extend(probs)\n        all_targets.extend(targets.cpu().numpy())\n\n        for true, pred, prob in zip(targets.cpu().numpy(), preds, probs):\n            validation_results.append([true, pred, prob])\n\n# ✅ 6. Precision, Recall 계산\nprecision = precision_score(all_targets, all_preds, zero_division=1)\nrecall = recall_score(all_targets, all_preds, zero_division=1)\n\n# ✅ 7. 결과 출력\nprint(f\"Test Precision: {precision:.4f}, Test Recall: {recall:.4f}\")\n\n# ✅ 8. 예측 결과 DataFrame 생성\ntest_results_df = pd.DataFrame(validation_results, columns=['Actual Target', 'Predicted Target', 'Probability'])\n# ✅ 6. 예측한 데이터 위치만 업데이트 (indices 활용)\nprint(f\"🔹 예측할 위치 개수: {len(indices)}\")\nprint(f\"🔹 예측된 데이터 개수: {len(test_results_df)}\")\n\n# ✅ 크기 불일치 확인 및 예측값 보정\nif len(indices) != len(test_results_df):\n    print(\"⚠️ `indices`와 `test_results_df` 크기가 다릅니다! 데이터 매칭 오류 가능성 있음.\")\n    print(f\"🔹 `indices` 크기: {len(indices)}, `test_results_df` 크기: {len(test_results_df)}\")\n\n    # ✅ 부족한 부분을 NaN으로 채움\n    while len(test_results_df) < len(indices):\n        test_results_df.loc[len(test_results_df)] = [None, None, None]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:35:10.084845Z","iopub.execute_input":"2025-02-03T09:35:10.085187Z","iopub.status.idle":"2025-02-03T09:35:14.472830Z","shell.execute_reply.started":"2025-02-03T09:35:10.085153Z","shell.execute_reply":"2025-02-03T09:35:14.471846Z"}},"outputs":[{"name":"stdout","text":"🔹 최종 입력 데이터 차원 (input_dim): 7\nTest Precision: 0.8200, Test Recall: 0.0790\n🔹 예측할 위치 개수: 1301\n🔹 예측된 데이터 개수: 1301\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-62-712d163d9c9a>:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# ✅ 1. 기존 dropped_test_df 복사 (1366개의 행 유지)\nmerged_test_df = dropped_test_df.copy()\n\n# ✅ 2. 'Predicted Target'과 'Probability' 열을 NaN으로 초기화\nmerged_test_df['Predicted Target'] = np.nan\nmerged_test_df['Probability'] = np.nan\n\n# ✅ 3. indices에 해당하는 행들만 업데이트\nmerged_test_df.loc[indices, ['Predicted Target', 'Probability']] = test_results_df[['Predicted Target', 'Probability']].values\n\ncols= ['End Change', '거래량', 'Target', 'Start Change', 'High Change', 'Low Change', '5 Day Diff',\t'20 Day Diff',\t'Sub ID']\nmerged_test_df = merged_test_df.merge(test_df, on = cols, how = 'inner')\n\n\n# ✅ 4. 최종 결과 확인\ndisplay(merged_test_df)\n\nmerged_test_df.to_csv('result_df.csv', encoding = 'utf-8-sig' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:35:14.473909Z","iopub.execute_input":"2025-02-03T09:35:14.474207Z","iopub.status.idle":"2025-02-03T09:35:14.527280Z","shell.execute_reply.started":"2025-02-03T09:35:14.474184Z","shell.execute_reply":"2025-02-03T09:35:14.526702Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"             거래량  End Change  Start Change  High Change  Low Change  \\\n0       297786.0      -14.06         -4.17         2.07      -10.76   \n1       201581.0        8.48         -0.61        10.00        0.00   \n2       212906.0        9.50          0.00         9.50        0.00   \n3        83053.0       -1.22          0.71         0.00       -3.75   \n4       849076.0       15.29         -2.27        17.97        0.00   \n...          ...         ...           ...          ...         ...   \n1794  39221617.0        6.72          5.72         5.92       -4.72   \n1795  59168257.0        6.04          3.40        12.25       -2.14   \n1796  28058840.0       -0.80          5.05         0.38       -8.78   \n1797  23920873.0       -6.95          0.24         1.05       -9.60   \n1798  32045682.0       -8.34        -12.08        16.11       -1.09   \n\n      5 Day Diff  20 Day Diff  Target      Sub ID  Predicted Target  \\\n0          20.85        30.00       1  199480-A-a               NaN   \n1           6.50        23.54       1  199480-A-a               NaN   \n2          -4.33        10.95       1  199480-A-a               NaN   \n3          -4.38        11.04       1  199480-A-a               NaN   \n4         -14.27        -4.01       1  199480-A-a               NaN   \n...          ...          ...     ...         ...               ...   \n1794       -4.39       -18.96       1    1470-E-a               0.0   \n1795       -7.31       -20.43       0    1470-E-a               0.0   \n1796       -2.41       -16.91       0    1470-E-a               0.0   \n1797        2.75        -7.96       0    1470-E-a               0.0   \n1798       11.20         3.07       0    1470-E-a               0.0   \n\n      Probability         일자       종목  \n0             NaN 2024-12-09  뱅크웨어글로벌  \n1             NaN 2024-12-10  뱅크웨어글로벌  \n2             NaN 2024-12-11  뱅크웨어글로벌  \n3             NaN 2024-12-12  뱅크웨어글로벌  \n4             NaN 2024-12-13  뱅크웨어글로벌  \n...           ...        ...      ...  \n1794     0.599347 2024-11-25     삼부토건  \n1795     0.487884 2024-11-26     삼부토건  \n1796     0.419720 2024-11-27     삼부토건  \n1797     0.415517 2024-11-28     삼부토건  \n1798     0.443150 2024-11-29     삼부토건  \n\n[1799 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>거래량</th>\n      <th>End Change</th>\n      <th>Start Change</th>\n      <th>High Change</th>\n      <th>Low Change</th>\n      <th>5 Day Diff</th>\n      <th>20 Day Diff</th>\n      <th>Target</th>\n      <th>Sub ID</th>\n      <th>Predicted Target</th>\n      <th>Probability</th>\n      <th>일자</th>\n      <th>종목</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>297786.0</td>\n      <td>-14.06</td>\n      <td>-4.17</td>\n      <td>2.07</td>\n      <td>-10.76</td>\n      <td>20.85</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-09</td>\n      <td>뱅크웨어글로벌</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>201581.0</td>\n      <td>8.48</td>\n      <td>-0.61</td>\n      <td>10.00</td>\n      <td>0.00</td>\n      <td>6.50</td>\n      <td>23.54</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-10</td>\n      <td>뱅크웨어글로벌</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>212906.0</td>\n      <td>9.50</td>\n      <td>0.00</td>\n      <td>9.50</td>\n      <td>0.00</td>\n      <td>-4.33</td>\n      <td>10.95</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-11</td>\n      <td>뱅크웨어글로벌</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>83053.0</td>\n      <td>-1.22</td>\n      <td>0.71</td>\n      <td>0.00</td>\n      <td>-3.75</td>\n      <td>-4.38</td>\n      <td>11.04</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-12</td>\n      <td>뱅크웨어글로벌</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>849076.0</td>\n      <td>15.29</td>\n      <td>-2.27</td>\n      <td>17.97</td>\n      <td>0.00</td>\n      <td>-14.27</td>\n      <td>-4.01</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-13</td>\n      <td>뱅크웨어글로벌</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1794</th>\n      <td>39221617.0</td>\n      <td>6.72</td>\n      <td>5.72</td>\n      <td>5.92</td>\n      <td>-4.72</td>\n      <td>-4.39</td>\n      <td>-18.96</td>\n      <td>1</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.599347</td>\n      <td>2024-11-25</td>\n      <td>삼부토건</td>\n    </tr>\n    <tr>\n      <th>1795</th>\n      <td>59168257.0</td>\n      <td>6.04</td>\n      <td>3.40</td>\n      <td>12.25</td>\n      <td>-2.14</td>\n      <td>-7.31</td>\n      <td>-20.43</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.487884</td>\n      <td>2024-11-26</td>\n      <td>삼부토건</td>\n    </tr>\n    <tr>\n      <th>1796</th>\n      <td>28058840.0</td>\n      <td>-0.80</td>\n      <td>5.05</td>\n      <td>0.38</td>\n      <td>-8.78</td>\n      <td>-2.41</td>\n      <td>-16.91</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.419720</td>\n      <td>2024-11-27</td>\n      <td>삼부토건</td>\n    </tr>\n    <tr>\n      <th>1797</th>\n      <td>23920873.0</td>\n      <td>-6.95</td>\n      <td>0.24</td>\n      <td>1.05</td>\n      <td>-9.60</td>\n      <td>2.75</td>\n      <td>-7.96</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.415517</td>\n      <td>2024-11-28</td>\n      <td>삼부토건</td>\n    </tr>\n    <tr>\n      <th>1798</th>\n      <td>32045682.0</td>\n      <td>-8.34</td>\n      <td>-12.08</td>\n      <td>16.11</td>\n      <td>-1.09</td>\n      <td>11.20</td>\n      <td>3.07</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.443150</td>\n      <td>2024-11-29</td>\n      <td>삼부토건</td>\n    </tr>\n  </tbody>\n</table>\n<p>1799 rows × 13 columns</p>\n</div>"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}