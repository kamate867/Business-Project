{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10590593,"sourceType":"datasetVersion","datasetId":6554525}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport string\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.preprocessing import StandardScaler\nimport string\nfrom concurrent.futures import ProcessPoolExecutor\nfrom time import time\nfrom numpy.lib.stride_tricks import sliding_window_view\nfrom collections import defaultdict\nfrom sklearn.model_selection import GroupShuffleSplit\nimport warnings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:44:46.894013Z","iopub.execute_input":"2025-02-03T09:44:46.894389Z","iopub.status.idle":"2025-02-03T09:44:53.994248Z","shell.execute_reply.started":"2025-02-03T09:44:46.894346Z","shell.execute_reply":"2025-02-03T09:44:53.992931Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nbase_dir = '/kaggle/input/stock-data'\n\nstockdata_list = []\nfor path in os.listdir(base_dir):\n    stockdata_list.append(os.path.join(base_dir, path))\n    stockdata_list = sorted(stockdata_list, key = lambda x: int(x.split('_')[-1].split('.')[0]))\n\nstock_df_list = []\nfor path in stockdata_list:\n    file_df = pd.read_csv(path, encoding = 'utf-8-sig', index_col = 0)\n    stock_df_list.append(file_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:44:53.995232Z","iopub.execute_input":"2025-02-03T09:44:53.995717Z","iopub.status.idle":"2025-02-03T09:45:04.990123Z","shell.execute_reply.started":"2025-02-03T09:44:53.995686Z","shell.execute_reply":"2025-02-03T09:45:04.988941Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n\ndef process_df(df):\n    final_group_list = []\n    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'], format='%Y-%m-%d')\n    numeric_cols = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰']\n    for col in numeric_cols:\n        if df[col].isnull().any():\n            df[col] = df[col].fillna(0).astype(float)\n\n    grouped_dfs = [group for _, group in df.groupby('ID')]\n    for group in grouped_dfs:\n            \n        group.dropna(inplace= True)\n        group = group.sort_values(by='ì¼ì', ascending=True).copy()\n        group.reset_index(drop=True, inplace=True)\n    \n    \n        group['End Change'] = (group['í˜„ì¬ê°€'] - group['í˜„ì¬ê°€'].shift(1)) / group['í˜„ì¬ê°€'].shift(1) * 100\n        group.dropna(subset= ['End Change'], inplace = True)\n        group.reset_index(drop=True, inplace=True)\n        condition_1 = (group['End Change'] > 29) & (group['End Change'] <= 31)\n        condition_2 = (\n                    (group['ê³ ê°€'].shift(-1) >= group['í˜„ì¬ê°€'] * 1.29) | \n                    (group['ê³ ê°€'].shift(-2) >= group['í˜„ì¬ê°€'] * 1.29)\n                )\n        valid_rows = condition_1 & condition_2\n        valid_indices = group.index[valid_rows]\n        start_indices = np.maximum(0, valid_indices - 35)\n        end_indices = np.minimum(valid_indices + 25, len(df))\n        \n        filtered_list = [\n            group.iloc[start:end]\n            for start, end in zip(start_indices, end_indices)\n        ]\n    \n        if filtered_list:\n            grouped_df = pd.concat(filtered_list, axis=0).drop_duplicates()\n            grouped_df = grouped_df.sort_values(by='ì¼ì', ascending=True).reset_index(drop=True)\n        else:\n            grouped_df = pd.DataFrame()  \n    \n        \n        if not grouped_df.empty:\n            grouped_df['Start Change'] = (grouped_df['ì‹œê°€'] - grouped_df['í˜„ì¬ê°€'].shift(1)) / grouped_df['í˜„ì¬ê°€'].shift(1) * 100\n            grouped_df['High Change'] = (grouped_df['ê³ ê°€'] - grouped_df['ì‹œê°€']) / grouped_df['ì‹œê°€'] * 100\n            grouped_df['Low Change'] = (grouped_df['ì €ê°€'] - grouped_df['ì‹œê°€']) / grouped_df['ì‹œê°€'] * 100\n            grouped_df['5 Day MA'] = grouped_df['í˜„ì¬ê°€'].rolling(window=5).mean()\n            grouped_df['20 Day MA'] = grouped_df['í˜„ì¬ê°€'].rolling(window=20).mean()\n            grouped_df['5 Day Diff'] = np.clip( (grouped_df['5 Day MA'] - grouped_df['í˜„ì¬ê°€']) / grouped_df['í˜„ì¬ê°€'] * 100, -30, 30 )\n            grouped_df['20 Day Diff'] = np.clip( (grouped_df['20 Day MA'] - grouped_df['í˜„ì¬ê°€']) / grouped_df['í˜„ì¬ê°€'] * 100, -30, 30)\n    \n            thresholds = grouped_df['í˜„ì¬ê°€'] * 1.12\n            high_array = grouped_df['ê³ ê°€'].to_numpy()\n            windows = sliding_window_view(high_array[1:], window_shape = 3)\n            rolling_max = np.max(windows, axis = 1) \n            rolling_max = np.concatenate([rolling_max, [np.nan] * 3])\n            grouped_df['Rolling Max'] = rolling_max\n            grouped_df['Target'] = (rolling_max >= thresholds).astype(int)\n        \n    \n            condition_1 = (grouped_df['End Change'] > 29) & (grouped_df['End Change'] <= 31)\n            condition_2 = (\n                                (grouped_df['ê³ ê°€'].shift(-1) >= grouped_df['í˜„ì¬ê°€'] * 1.29) | \n                                (grouped_df['ê³ ê°€'].shift(-2) >= grouped_df['í˜„ì¬ê°€'] * 1.29)\n                            )\n            valid_rows = condition_1 & condition_2\n            valid_indices = grouped_df.index[valid_rows]\n            start_indices = np.maximum(0, valid_indices - 5)\n            end_indices = np.minimum(valid_indices + 15, len(grouped_df))\n            \n            subgroup_list = [\n                grouped_df.iloc[start:end+1] \n                for start, end in zip(start_indices, end_indices)\n            ]\n        \n           \n        \n            if len(subgroup_list) >= 2:\n                i = 0\n                while i < len(subgroup_list) - 1:\n                    df_1 = subgroup_list[i]\n                    df_2 = subgroup_list[i+1]\n                    if not df_1.merge(df_2, how = 'inner').empty:\n                        new_df = pd.concat([df_1, df_2], axis = 0 ).drop_duplicates()\n                        subgroup_list[i] = new_df\n                        subgroup_list.pop(i+1)\n        \n                    else:\n                        i += 1\n            updated_subgroup_list = []\n            uppercase = list(string.ascii_uppercase)\n            lowercase = list(string.ascii_lowercase)\n            i, j = 0, 0\n            for subgroup in subgroup_list:\n                subgroup = subgroup.copy()\n                subgroup['Sub ID'] = f\"{subgroup['ID'].iloc[0]}-{uppercase[i]}-{lowercase[j]}\"\n                updated_subgroup_list.append(subgroup)\n                i += 1\n                if i == 25:\n                    i = 0\n                    j += 1\n        \n        \n            final_df = pd.concat(updated_subgroup_list, axis=0) if updated_subgroup_list else pd.DataFrame()\n            final_df = final_df[final_df['ì¼ì'] >= pd.to_datetime('20150701')]\n            final_df.reset_index(drop=True, inplace=True)\n            '''\n            subgrouped_dfs = []\n            for sub_id, group in final_df.groupby('Sub ID'): \n                group = group.copy()  \n                ss_scaler = StandardScaler()  \n                group['Trade Amount'] = ss_scaler.fit_transform(group[['ê±°ë˜ëŸ‰']]) * 10  \n                subgrouped_dfs.append(group)  \n                \n            final_df = pd.concat(subgrouped_dfs, axis=0)\n            '''\n            columns_to_round = ['End Change', 'Start Change', 'High Change', 'Low Change', '5 Day Diff', '20 Day Diff']\n            final_df.loc[:, columns_to_round] = final_df.loc[:, columns_to_round].round(2)\n            final_df.drop(columns = ['Rolling Max','ì‹œê°€', 'ê³ ê°€','ì €ê°€','í˜„ì¬ê°€', 'ID', '5 Day MA', '20 Day MA'], inplace = True)\n            final_group_list.append(final_df)\n    return final_group_list\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:07.803563Z","iopub.execute_input":"2025-02-03T09:46:07.804037Z","iopub.status.idle":"2025-02-03T09:46:07.825633Z","shell.execute_reply.started":"2025-02-03T09:46:07.804007Z","shell.execute_reply":"2025-02-03T09:46:07.824031Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def process_df_parallel(stock_df_list):\n    with ProcessPoolExecutor() as executor:\n        processed_df_list = list(executor.map(process_df, stock_df_list))\n    return processed_df_list\n\nprocessed_df_list = process_df_parallel(stock_df_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:08.237377Z","iopub.execute_input":"2025-02-03T09:46:08.237798Z","iopub.status.idle":"2025-02-03T09:46:26.653160Z","shell.execute_reply.started":"2025-02-03T09:46:08.237765Z","shell.execute_reply":"2025-02-03T09:46:26.651654Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"temp_list = [ item for sublist in processed_df_list for item in sublist]\nscaled_df = pd.concat(temp_list, axis = 0)\nna_df = scaled_df[scaled_df['Start Change'].isna()]\ngroups_with_na = scaled_df.groupby('Sub ID').apply(lambda group: group.isna().any().any())\ngroups_with_na = groups_with_na[groups_with_na].index\ncleaned_df = scaled_df[~scaled_df['Sub ID'].isin(groups_with_na)]\nscaled_df = cleaned_df.copy()\n#test_df = scaled_df[scaled_df['ì¼ì'] >= pd.to_datetime('2024-10-01')]\n#scaled_df = scaled_df[scaled_df['ì¼ì'] < pd.to_datetime('2024-10-01')]\nscaled_size= scaled_df.groupby('Sub ID').size()\n#test_size= test_df.groupby('Sub ID').size()\nscaled_idx = scaled_size[scaled_size >= 12].index\n#test_idx = test_size[test_size >= 12].index\nscaled_df = scaled_df.loc[scaled_df['Sub ID'].isin(scaled_idx)]\n#test_df = test_df.loc[test_df['Sub ID'].isin(test_idx)]\n#scaled_df.drop(columns = ['ì¼ì', 'ì¢…ëª©'], inplace = True)\n#dropped_test_df = test_df.drop(columns = ['ì¼ì', 'ì¢…ëª©'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:41.149566Z","iopub.execute_input":"2025-02-03T09:46:41.150053Z","iopub.status.idle":"2025-02-03T09:46:42.160520Z","shell.execute_reply.started":"2025-02-03T09:46:41.150010Z","shell.execute_reply":"2025-02-03T09:46:42.159181Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-7eeb27b09383>:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  groups_with_na = scaled_df.groupby('Sub ID').apply(lambda group: group.isna().any().any())\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(len(scaled_df['Sub ID'].unique()))\nprint(scaled_df['Target'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:46:45.024125Z","iopub.execute_input":"2025-02-03T09:46:45.024457Z","iopub.status.idle":"2025-02-03T09:46:45.035193Z","shell.execute_reply.started":"2025-02-03T09:46:45.024432Z","shell.execute_reply":"2025-02-03T09:46:45.033961Z"}},"outputs":[{"name":"stdout","text":"1282\nTarget\n0    14828\n1    13580\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"scaled_df.to_csv('what I need.csv', encoding = 'utf-8-sig')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:47:16.906869Z","iopub.execute_input":"2025-02-03T09:47:16.907277Z","iopub.status.idle":"2025-02-03T09:47:17.165764Z","shell.execute_reply.started":"2025-02-03T09:47:16.907249Z","shell.execute_reply":"2025-02-03T09:47:17.164560Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def generate_grouped_sequences(df, group_col, target_col, min_seq_length=7):\n    sequences = []\n    grouped = df.groupby(group_col)\n\n    for sub_id, group in grouped:\n        group = group.reset_index(drop=True)\n\n        # ìµœì†Œ ê¸¸ì´ê°€ ì¶©ì¡±ë˜ë©´ í•œ ë²ˆì€ ë°˜ë“œì‹œ ì‹¤í–‰\n        for seq_end in range(min_seq_length, len(group) + 1):  # âœ… +1 ì¶”ê°€ë¡œ ìµœì†Œ ê¸¸ì´ë„ í¬í•¨\n            seq = group.iloc[seq_end - min_seq_length:seq_end].drop(columns=[group_col, target_col]).copy()\n\n            if 'ê±°ë˜ëŸ‰' in seq.columns:\n                ss_scaler = StandardScaler()\n                seq['Trade Amount'] = ss_scaler.fit_transform(seq[['ê±°ë˜ëŸ‰']]) * 10  \n                seq['Trade Amount'] = seq['Trade Amount'].round(2)\n                seq.drop(columns=['ê±°ë˜ëŸ‰'], inplace=True)\n\n            seq = seq.to_numpy(dtype=np.float32)\n\n            target = group.iloc[seq_end - 1][target_col]  # âœ… ì˜¬ë°”ë¥¸ target ìœ„ì¹˜ ì§€ì •\n\n            sequences.append((seq, target, sub_id))\n\n    return sequences\n\n\n\ndef split_data_by_group(sequences, group_col):\n\n    groups = [seq[2] for seq in sequences] \n    gss = GroupShuffleSplit(test_size = 0.2, n_splits =1, random_state = 42)\n\n    splits = []\n\n    for train_indices, val_indices in gss.split(sequences, groups = groups):\n        train_data = [sequences[i] for i in train_indices]\n        val_data = [sequences[i] for i in val_indices]\n        splits.append((train_data, val_data))\n    \n    return splits\n\n\nclass SubIDGroupedDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq, target, sub_id = self.data[idx]\n        return torch.tensor(seq, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n    targets = torch.stack(targets)\n    return padded_inputs, targets\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.3):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)  # (batch_size, seq_length, hidden_dim)\n        x = lstm_out[:, -1, :]  # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©\n        x = self.fc(x)  # ìµœì¢… ì¶œë ¥\n        return x\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=2, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, outputs, targets):\n        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(outputs, targets)\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:31:44.247719Z","iopub.execute_input":"2025-02-03T09:31:44.247973Z","iopub.status.idle":"2025-02-03T09:31:44.263608Z","shell.execute_reply.started":"2025-02-03T09:31:44.247942Z","shell.execute_reply":"2025-02-03T09:31:44.262703Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"sequences = generate_grouped_sequences(\n    scaled_df, group_col='Sub ID', target_col='Target', min_seq_length=7\n)\nsplits = split_data_by_group(sequences, group_col = 'Sub ID')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:31:44.264524Z","iopub.execute_input":"2025-02-03T09:31:44.264820Z","iopub.status.idle":"2025-02-03T09:32:50.755132Z","shell.execute_reply.started":"2025-02-03T09:31:44.264792Z","shell.execute_reply":"2025-02-03T09:32:50.754397Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"train_data, val_data = splits[0]\ntrain_dataset = SubIDGroupedDataset(train_data)\nval_dataset = SubIDGroupedDataset(val_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\ninput_dim = scaled_df.shape[1] - 2 \nhidden_dim = 512\nnum_layers = 5\noutput_dim = 1  # ì´ì§„ ë¶„ë¥˜\n\nmodel = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout=0)\n\n# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\ncriterion = FocalLoss() \noptimizer = optim.Adam(model.parameters(), lr=1e-4 )\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 15\nmodel_save_dir = \"saved_models_1\"\nos.makedirs(model_save_dir, exist_ok=True)  # ëª¨ë¸ ì €ì¥ í´ë” ìƒì„±\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        targets = targets.float()\n\n        optimizer.zero_grad()\n        outputs = model(inputs).squeeze(1)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n    # ê²€ì¦\n    model.eval()\n    total_val_loss = 0\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs).squeeze(1)\n            loss = criterion(outputs, targets)\n            total_val_loss += loss.item()\n            preds = (torch.sigmoid(outputs) > 0.65).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(targets.cpu().numpy())\n\n    precision = precision_score(all_targets, all_preds, zero_division=1)\n    recall = recall_score(all_targets, all_preds, zero_division=1)\n    scheduler.step(total_val_loss)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, \"\n          f\"Val Loss: {total_val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥\n    model_save_path = os.path.join(model_save_dir, f\"model_epoch_{epoch+1}.pth\")\n    torch.save(model.state_dict(), model_save_path)\n\nprint(\"\\n===== ëª¨ë“  ì—í¬í¬ ëª¨ë¸ ì €ì¥ ì™„ë£Œ =====\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:32:50.757472Z","iopub.execute_input":"2025-02-03T09:32:50.757738Z","iopub.status.idle":"2025-02-03T09:34:06.598285Z","shell.execute_reply.started":"2025-02-03T09:32:50.757715Z","shell.execute_reply":"2025-02-03T09:34:06.597234Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Train Loss: 156.5791, Val Loss: 38.3304, Precision: 1.0000, Recall: 0.0000\nEpoch 2/15, Train Loss: 151.1154, Val Loss: 37.6584, Precision: 0.8947, Recall: 0.0120\nEpoch 3/15, Train Loss: 149.7340, Val Loss: 37.4354, Precision: 0.8791, Recall: 0.0565\nEpoch 4/15, Train Loss: 148.7586, Val Loss: 37.3810, Precision: 1.0000, Recall: 0.0014\nEpoch 5/15, Train Loss: 148.3853, Val Loss: 37.5712, Precision: 0.9000, Recall: 0.0381\nEpoch 6/15, Train Loss: 147.4862, Val Loss: 37.8461, Precision: 0.8788, Recall: 0.0614\nEpoch 7/15, Train Loss: 146.7066, Val Loss: 37.6591, Precision: 0.8519, Recall: 0.0974\nEpoch 8/15, Train Loss: 144.9141, Val Loss: 37.4829, Precision: 0.8692, Recall: 0.0797\nEpoch 9/15, Train Loss: 143.7939, Val Loss: 37.5530, Precision: 0.8661, Recall: 0.0776\nEpoch 10/15, Train Loss: 142.8494, Val Loss: 38.1004, Precision: 0.8509, Recall: 0.0685\nEpoch 11/15, Train Loss: 140.6478, Val Loss: 38.6033, Precision: 0.7460, Recall: 0.0995\nEpoch 12/15, Train Loss: 139.4359, Val Loss: 38.6358, Precision: 0.8480, Recall: 0.0748\nEpoch 13/15, Train Loss: 138.4612, Val Loss: 39.0380, Precision: 0.7604, Recall: 0.1164\nEpoch 14/15, Train Loss: 136.7338, Val Loss: 39.8541, Precision: 0.7264, Recall: 0.1087\nEpoch 15/15, Train Loss: 135.8474, Val Loss: 39.9484, Precision: 0.7206, Recall: 0.1037\n\n===== ëª¨ë“  ì—í¬í¬ ëª¨ë¸ ì €ì¥ ì™„ë£Œ =====\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"train_data, val_data = splits[0]\ntrain_dataset = SubIDGroupedDataset(train_data)\nval_dataset = SubIDGroupedDataset(val_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n\ninput_dim = scaled_df.shape[1] - 2 \nhidden_dim =128\nnum_layers =4\noutput_dim = 1  # ì´ì§„ ë¶„ë¥˜\n\nmodel = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout=0.2)\n\n# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\ncriterion = FocalLoss() \noptimizer = optim.Adam(model.parameters(), lr=1e-4 *2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 15\nmodel_save_dir = \"saved_models_2\"\nos.makedirs(model_save_dir, exist_ok=True)  # ëª¨ë¸ ì €ì¥ í´ë” ìƒì„±\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        targets = targets.float()\n\n        optimizer.zero_grad()\n        outputs = model(inputs).squeeze(1)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n    # ê²€ì¦\n    model.eval()\n    total_val_loss = 0\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs).squeeze(1)\n            loss = criterion(outputs, targets)\n            total_val_loss += loss.item()\n            preds = (torch.sigmoid(outputs) > 0.65).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(targets.cpu().numpy())\n\n    precision = precision_score(all_targets, all_preds, zero_division=1)\n    recall = recall_score(all_targets, all_preds, zero_division=1)\n    scheduler.step(total_val_loss)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, \"\n          f\"Val Loss: {total_val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥\n    model_save_path = os.path.join(model_save_dir, f\"model_epoch_{epoch+1}.pth\")\n    torch.save(model.state_dict(), model_save_path)\n\nprint(\"\\n===== ëª¨ë“  ì—í¬í¬ ëª¨ë¸ ì €ì¥ ì™„ë£Œ =====\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:34:06.599432Z","iopub.execute_input":"2025-02-03T09:34:06.599652Z","iopub.status.idle":"2025-02-03T09:35:10.083648Z","shell.execute_reply.started":"2025-02-03T09:34:06.599634Z","shell.execute_reply":"2025-02-03T09:35:10.082724Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Train Loss: 309.9163, Val Loss: 75.2983, Precision: 0.8302, Recall: 0.0311\nEpoch 2/15, Train Loss: 300.7168, Val Loss: 75.2464, Precision: 0.8846, Recall: 0.0487\nEpoch 3/15, Train Loss: 298.7589, Val Loss: 74.8806, Precision: 0.8679, Recall: 0.0325\nEpoch 4/15, Train Loss: 296.6977, Val Loss: 74.3986, Precision: 0.9348, Recall: 0.0303\nEpoch 5/15, Train Loss: 294.7629, Val Loss: 75.4754, Precision: 0.8417, Recall: 0.0713\nEpoch 6/15, Train Loss: 293.0445, Val Loss: 75.0407, Precision: 0.8852, Recall: 0.0381\nEpoch 7/15, Train Loss: 290.5242, Val Loss: 75.8473, Precision: 0.8750, Recall: 0.0148\nEpoch 8/15, Train Loss: 286.9823, Val Loss: 76.0934, Precision: 0.7698, Recall: 0.0755\nEpoch 9/15, Train Loss: 285.0146, Val Loss: 76.5017, Precision: 0.8182, Recall: 0.0572\nEpoch 10/15, Train Loss: 283.4439, Val Loss: 76.5853, Precision: 0.7610, Recall: 0.0854\nEpoch 11/15, Train Loss: 280.5492, Val Loss: 76.7328, Precision: 0.8049, Recall: 0.0699\nEpoch 12/15, Train Loss: 278.2431, Val Loss: 78.0916, Precision: 0.7636, Recall: 0.0593\nEpoch 13/15, Train Loss: 277.6566, Val Loss: 77.7588, Precision: 0.7730, Recall: 0.0769\nEpoch 14/15, Train Loss: 276.0939, Val Loss: 77.7615, Precision: 0.7937, Recall: 0.0706\nEpoch 15/15, Train Loss: 275.4985, Val Loss: 78.4566, Precision: 0.7548, Recall: 0.0826\n\n===== ëª¨ë“  ì—í¬í¬ ëª¨ë¸ ì €ì¥ ì™„ë£Œ =====\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"scaled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:36:45.486579Z","iopub.execute_input":"2025-02-03T09:36:45.486925Z","iopub.status.idle":"2025-02-03T09:36:45.504262Z","shell.execute_reply.started":"2025-02-03T09:36:45.486899Z","shell.execute_reply":"2025-02-03T09:36:45.503278Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"          ê±°ë˜ëŸ‰  End Change  Start Change  High Change  Low Change  5 Day Diff  \\\n0    356243.0       -2.94         -0.32         1.37       -2.95        6.80   \n1    215563.0        0.76          0.00         2.71       -0.33        3.42   \n2   2638761.0       10.11          0.00        15.70        0.00       -6.07   \n3    672884.0       -0.98          1.56         2.88       -5.77       -4.50   \n4    255844.0       -2.96         -0.99         1.39       -2.39       -0.91   \n..        ...         ...           ...          ...         ...         ...   \n16    39306.0        0.48         -0.94         2.39       -1.91        1.70   \n17    33877.0        2.83         -0.47         4.27       -1.42       -1.28   \n18    25727.0       -2.29          0.46         0.00       -4.11        0.28   \n19    53288.0       -1.88          0.00         0.46       -4.23        1.72   \n20    40788.0       -1.91         -0.48         1.92       -1.92        3.12   \n\n    20 Day Diff  Target      Sub ID  \n0         23.85       1  373110-A-a  \n1         20.78       1  373110-A-a  \n2          8.25       1  373110-A-a  \n3          7.63       1  373110-A-a  \n4          9.35       1  373110-A-a  \n..          ...     ...         ...  \n16        11.70       0    2420-A-a  \n17         7.96       0    2420-A-a  \n18         9.69       0    2420-A-a  \n19        10.88       0    2420-A-a  \n20        12.02       0    2420-A-a  \n\n[26457 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ê±°ë˜ëŸ‰</th>\n      <th>End Change</th>\n      <th>Start Change</th>\n      <th>High Change</th>\n      <th>Low Change</th>\n      <th>5 Day Diff</th>\n      <th>20 Day Diff</th>\n      <th>Target</th>\n      <th>Sub ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>356243.0</td>\n      <td>-2.94</td>\n      <td>-0.32</td>\n      <td>1.37</td>\n      <td>-2.95</td>\n      <td>6.80</td>\n      <td>23.85</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>215563.0</td>\n      <td>0.76</td>\n      <td>0.00</td>\n      <td>2.71</td>\n      <td>-0.33</td>\n      <td>3.42</td>\n      <td>20.78</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2638761.0</td>\n      <td>10.11</td>\n      <td>0.00</td>\n      <td>15.70</td>\n      <td>0.00</td>\n      <td>-6.07</td>\n      <td>8.25</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>672884.0</td>\n      <td>-0.98</td>\n      <td>1.56</td>\n      <td>2.88</td>\n      <td>-5.77</td>\n      <td>-4.50</td>\n      <td>7.63</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>255844.0</td>\n      <td>-2.96</td>\n      <td>-0.99</td>\n      <td>1.39</td>\n      <td>-2.39</td>\n      <td>-0.91</td>\n      <td>9.35</td>\n      <td>1</td>\n      <td>373110-A-a</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>39306.0</td>\n      <td>0.48</td>\n      <td>-0.94</td>\n      <td>2.39</td>\n      <td>-1.91</td>\n      <td>1.70</td>\n      <td>11.70</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>33877.0</td>\n      <td>2.83</td>\n      <td>-0.47</td>\n      <td>4.27</td>\n      <td>-1.42</td>\n      <td>-1.28</td>\n      <td>7.96</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>25727.0</td>\n      <td>-2.29</td>\n      <td>0.46</td>\n      <td>0.00</td>\n      <td>-4.11</td>\n      <td>0.28</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>53288.0</td>\n      <td>-1.88</td>\n      <td>0.00</td>\n      <td>0.46</td>\n      <td>-4.23</td>\n      <td>1.72</td>\n      <td>10.88</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>40788.0</td>\n      <td>-1.91</td>\n      <td>-0.48</td>\n      <td>1.92</td>\n      <td>-1.92</td>\n      <td>3.12</td>\n      <td>12.02</td>\n      <td>0</td>\n      <td>2420-A-a</td>\n    </tr>\n  </tbody>\n</table>\n<p>26457 rows Ã— 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"dropped_test_df.reset_index(drop = True, inplace = True)\ndef generate_grouped_sequences(df, group_col, target_col, min_seq_length=7):\n    sequences = []\n    indices = []  # âœ… ì˜ˆì¸¡í•  ë°ì´í„°ì˜ ì›ë³¸ ì¸ë±ìŠ¤ ì €ì¥\n    \n    df = df.copy()  # ì›ë³¸ DataFrame ìœ ì§€\n    df[\"original_index\"] = df.index  # âœ… ì›ë˜ ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œìš´ ì—´ë¡œ ì €ì¥\n    \n    grouped = df.groupby(group_col)\n\n    for sub_id, group in grouped:\n        group = group.reset_index(drop=True)  # âœ… ì—¬ê¸°ì„œ reset_index() í•˜ë©´ original_indexëŠ” ìœ ì§€ë¨\n        \n        if len(group) >= min_seq_length:\n            for seq_end in range(min_seq_length, len(group) + 1):  \n                # âœ… í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œì™¸í•˜ê³ , ì •í™•í•˜ê²Œ 7ê°œì˜ Featureë§Œ ì‚¬ìš©\n                if 'ê±°ë˜ëŸ‰' in group.columns:\n                    ss_scaler = StandardScaler()\n                    group['Trade Amount'] = ss_scaler.fit_transform(group[['ê±°ë˜ëŸ‰']]) * 10  \n                    group['Trade Amount'] = group['Trade Amount'].round(2)\n\n                feature_cols = ['End Change', 'Start Change', 'High Change', 'Low Change', '5 Day Diff', '20 Day Diff', 'Trade Amount']\n                seq = group.iloc[seq_end - min_seq_length:seq_end][feature_cols].copy()\n                seq = seq.to_numpy(dtype=np.float32)\n\n                target = group.iloc[seq_end - 1][target_col]  \n                original_index = group.iloc[seq_end - 1]['original_index']  # âœ… ì›ë˜ dropped_test_dfì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜´\n\n                sequences.append((seq, target, sub_id))\n                indices.append(original_index)\n\n    # âœ… input_dimì„ ê°•ì œì ìœ¼ë¡œ 7ë¡œ ë§ì¶”ì—ˆëŠ”ì§€ í™•ì¸\n    input_dim = sequences[0][0].shape[-1] if sequences else 0\n    print(f\"ğŸ”¹ ìµœì¢… ì…ë ¥ ë°ì´í„° ì°¨ì› (input_dim): {input_dim}\")\n\n    return sequences, indices, input_dim\n\n\n\n\n# âœ… 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (ìˆ˜ì •ëœ `generate_grouped_sequences` ì ìš©)\nsequences, indices, input_dim = generate_grouped_sequences(dropped_test_df, group_col='Sub ID', target_col='Target', min_seq_length=7)\ntest_dataset = SubIDGroupedDataset(sequences)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n\ninput_dim = 7  # âœ… ì €ì¥ëœ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë³€ê²½\nhidden_dim =128\nnum_layers =4 #âœ… ì €ì¥ëœ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë³€ê²½\noutput_dim = 1  \n\nmodel = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout=0.2).to(device)\n\n# âœ… 4. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\nmodel_path = \"/kaggle/working/saved_models_2/model_epoch_9.pth\"\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\n# âœ… 5. ëª¨ë¸ í‰ê°€ (ì˜ˆì¸¡ í™•ë¥  í¬í•¨)\nall_preds, all_targets, all_probs = [], [], []\nvalidation_results = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs).squeeze(1)\n\n        probs = torch.sigmoid(outputs).cpu().numpy()  # í™•ë¥  ê³„ì‚°\n        preds = (probs > 0.65).astype(int)  # ì˜ˆì¸¡ ê²°ê³¼\n\n        all_preds.extend(preds)\n        all_probs.extend(probs)\n        all_targets.extend(targets.cpu().numpy())\n\n        for true, pred, prob in zip(targets.cpu().numpy(), preds, probs):\n            validation_results.append([true, pred, prob])\n\n# âœ… 6. Precision, Recall ê³„ì‚°\nprecision = precision_score(all_targets, all_preds, zero_division=1)\nrecall = recall_score(all_targets, all_preds, zero_division=1)\n\n# âœ… 7. ê²°ê³¼ ì¶œë ¥\nprint(f\"Test Precision: {precision:.4f}, Test Recall: {recall:.4f}\")\n\n# âœ… 8. ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±\ntest_results_df = pd.DataFrame(validation_results, columns=['Actual Target', 'Predicted Target', 'Probability'])\n# âœ… 6. ì˜ˆì¸¡í•œ ë°ì´í„° ìœ„ì¹˜ë§Œ ì—…ë°ì´íŠ¸ (indices í™œìš©)\nprint(f\"ğŸ”¹ ì˜ˆì¸¡í•  ìœ„ì¹˜ ê°œìˆ˜: {len(indices)}\")\nprint(f\"ğŸ”¹ ì˜ˆì¸¡ëœ ë°ì´í„° ê°œìˆ˜: {len(test_results_df)}\")\n\n# âœ… í¬ê¸° ë¶ˆì¼ì¹˜ í™•ì¸ ë° ì˜ˆì¸¡ê°’ ë³´ì •\nif len(indices) != len(test_results_df):\n    print(\"âš ï¸ `indices`ì™€ `test_results_df` í¬ê¸°ê°€ ë‹¤ë¦…ë‹ˆë‹¤! ë°ì´í„° ë§¤ì¹­ ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìŒ.\")\n    print(f\"ğŸ”¹ `indices` í¬ê¸°: {len(indices)}, `test_results_df` í¬ê¸°: {len(test_results_df)}\")\n\n    # âœ… ë¶€ì¡±í•œ ë¶€ë¶„ì„ NaNìœ¼ë¡œ ì±„ì›€\n    while len(test_results_df) < len(indices):\n        test_results_df.loc[len(test_results_df)] = [None, None, None]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:35:10.084845Z","iopub.execute_input":"2025-02-03T09:35:10.085187Z","iopub.status.idle":"2025-02-03T09:35:14.472830Z","shell.execute_reply.started":"2025-02-03T09:35:10.085153Z","shell.execute_reply":"2025-02-03T09:35:14.471846Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¹ ìµœì¢… ì…ë ¥ ë°ì´í„° ì°¨ì› (input_dim): 7\nTest Precision: 0.8200, Test Recall: 0.0790\nğŸ”¹ ì˜ˆì¸¡í•  ìœ„ì¹˜ ê°œìˆ˜: 1301\nğŸ”¹ ì˜ˆì¸¡ëœ ë°ì´í„° ê°œìˆ˜: 1301\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-62-712d163d9c9a>:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# âœ… 1. ê¸°ì¡´ dropped_test_df ë³µì‚¬ (1366ê°œì˜ í–‰ ìœ ì§€)\nmerged_test_df = dropped_test_df.copy()\n\n# âœ… 2. 'Predicted Target'ê³¼ 'Probability' ì—´ì„ NaNìœ¼ë¡œ ì´ˆê¸°í™”\nmerged_test_df['Predicted Target'] = np.nan\nmerged_test_df['Probability'] = np.nan\n\n# âœ… 3. indicesì— í•´ë‹¹í•˜ëŠ” í–‰ë“¤ë§Œ ì—…ë°ì´íŠ¸\nmerged_test_df.loc[indices, ['Predicted Target', 'Probability']] = test_results_df[['Predicted Target', 'Probability']].values\n\ncols= ['End Change', 'ê±°ë˜ëŸ‰', 'Target', 'Start Change', 'High Change', 'Low Change', '5 Day Diff',\t'20 Day Diff',\t'Sub ID']\nmerged_test_df = merged_test_df.merge(test_df, on = cols, how = 'inner')\n\n\n# âœ… 4. ìµœì¢… ê²°ê³¼ í™•ì¸\ndisplay(merged_test_df)\n\nmerged_test_df.to_csv('result_df.csv', encoding = 'utf-8-sig' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:35:14.473909Z","iopub.execute_input":"2025-02-03T09:35:14.474207Z","iopub.status.idle":"2025-02-03T09:35:14.527280Z","shell.execute_reply.started":"2025-02-03T09:35:14.474184Z","shell.execute_reply":"2025-02-03T09:35:14.526702Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"             ê±°ë˜ëŸ‰  End Change  Start Change  High Change  Low Change  \\\n0       297786.0      -14.06         -4.17         2.07      -10.76   \n1       201581.0        8.48         -0.61        10.00        0.00   \n2       212906.0        9.50          0.00         9.50        0.00   \n3        83053.0       -1.22          0.71         0.00       -3.75   \n4       849076.0       15.29         -2.27        17.97        0.00   \n...          ...         ...           ...          ...         ...   \n1794  39221617.0        6.72          5.72         5.92       -4.72   \n1795  59168257.0        6.04          3.40        12.25       -2.14   \n1796  28058840.0       -0.80          5.05         0.38       -8.78   \n1797  23920873.0       -6.95          0.24         1.05       -9.60   \n1798  32045682.0       -8.34        -12.08        16.11       -1.09   \n\n      5 Day Diff  20 Day Diff  Target      Sub ID  Predicted Target  \\\n0          20.85        30.00       1  199480-A-a               NaN   \n1           6.50        23.54       1  199480-A-a               NaN   \n2          -4.33        10.95       1  199480-A-a               NaN   \n3          -4.38        11.04       1  199480-A-a               NaN   \n4         -14.27        -4.01       1  199480-A-a               NaN   \n...          ...          ...     ...         ...               ...   \n1794       -4.39       -18.96       1    1470-E-a               0.0   \n1795       -7.31       -20.43       0    1470-E-a               0.0   \n1796       -2.41       -16.91       0    1470-E-a               0.0   \n1797        2.75        -7.96       0    1470-E-a               0.0   \n1798       11.20         3.07       0    1470-E-a               0.0   \n\n      Probability         ì¼ì       ì¢…ëª©  \n0             NaN 2024-12-09  ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ  \n1             NaN 2024-12-10  ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ  \n2             NaN 2024-12-11  ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ  \n3             NaN 2024-12-12  ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ  \n4             NaN 2024-12-13  ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ  \n...           ...        ...      ...  \n1794     0.599347 2024-11-25     ì‚¼ë¶€í† ê±´  \n1795     0.487884 2024-11-26     ì‚¼ë¶€í† ê±´  \n1796     0.419720 2024-11-27     ì‚¼ë¶€í† ê±´  \n1797     0.415517 2024-11-28     ì‚¼ë¶€í† ê±´  \n1798     0.443150 2024-11-29     ì‚¼ë¶€í† ê±´  \n\n[1799 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ê±°ë˜ëŸ‰</th>\n      <th>End Change</th>\n      <th>Start Change</th>\n      <th>High Change</th>\n      <th>Low Change</th>\n      <th>5 Day Diff</th>\n      <th>20 Day Diff</th>\n      <th>Target</th>\n      <th>Sub ID</th>\n      <th>Predicted Target</th>\n      <th>Probability</th>\n      <th>ì¼ì</th>\n      <th>ì¢…ëª©</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>297786.0</td>\n      <td>-14.06</td>\n      <td>-4.17</td>\n      <td>2.07</td>\n      <td>-10.76</td>\n      <td>20.85</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-09</td>\n      <td>ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>201581.0</td>\n      <td>8.48</td>\n      <td>-0.61</td>\n      <td>10.00</td>\n      <td>0.00</td>\n      <td>6.50</td>\n      <td>23.54</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-10</td>\n      <td>ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>212906.0</td>\n      <td>9.50</td>\n      <td>0.00</td>\n      <td>9.50</td>\n      <td>0.00</td>\n      <td>-4.33</td>\n      <td>10.95</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-11</td>\n      <td>ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>83053.0</td>\n      <td>-1.22</td>\n      <td>0.71</td>\n      <td>0.00</td>\n      <td>-3.75</td>\n      <td>-4.38</td>\n      <td>11.04</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-12</td>\n      <td>ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>849076.0</td>\n      <td>15.29</td>\n      <td>-2.27</td>\n      <td>17.97</td>\n      <td>0.00</td>\n      <td>-14.27</td>\n      <td>-4.01</td>\n      <td>1</td>\n      <td>199480-A-a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2024-12-13</td>\n      <td>ë±…í¬ì›¨ì–´ê¸€ë¡œë²Œ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1794</th>\n      <td>39221617.0</td>\n      <td>6.72</td>\n      <td>5.72</td>\n      <td>5.92</td>\n      <td>-4.72</td>\n      <td>-4.39</td>\n      <td>-18.96</td>\n      <td>1</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.599347</td>\n      <td>2024-11-25</td>\n      <td>ì‚¼ë¶€í† ê±´</td>\n    </tr>\n    <tr>\n      <th>1795</th>\n      <td>59168257.0</td>\n      <td>6.04</td>\n      <td>3.40</td>\n      <td>12.25</td>\n      <td>-2.14</td>\n      <td>-7.31</td>\n      <td>-20.43</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.487884</td>\n      <td>2024-11-26</td>\n      <td>ì‚¼ë¶€í† ê±´</td>\n    </tr>\n    <tr>\n      <th>1796</th>\n      <td>28058840.0</td>\n      <td>-0.80</td>\n      <td>5.05</td>\n      <td>0.38</td>\n      <td>-8.78</td>\n      <td>-2.41</td>\n      <td>-16.91</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.419720</td>\n      <td>2024-11-27</td>\n      <td>ì‚¼ë¶€í† ê±´</td>\n    </tr>\n    <tr>\n      <th>1797</th>\n      <td>23920873.0</td>\n      <td>-6.95</td>\n      <td>0.24</td>\n      <td>1.05</td>\n      <td>-9.60</td>\n      <td>2.75</td>\n      <td>-7.96</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.415517</td>\n      <td>2024-11-28</td>\n      <td>ì‚¼ë¶€í† ê±´</td>\n    </tr>\n    <tr>\n      <th>1798</th>\n      <td>32045682.0</td>\n      <td>-8.34</td>\n      <td>-12.08</td>\n      <td>16.11</td>\n      <td>-1.09</td>\n      <td>11.20</td>\n      <td>3.07</td>\n      <td>0</td>\n      <td>1470-E-a</td>\n      <td>0.0</td>\n      <td>0.443150</td>\n      <td>2024-11-29</td>\n      <td>ì‚¼ë¶€í† ê±´</td>\n    </tr>\n  </tbody>\n</table>\n<p>1799 rows Ã— 13 columns</p>\n</div>"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}