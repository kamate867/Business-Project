{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1472453,"sourceType":"datasetVersion","datasetId":863934},{"sourceId":10727682,"sourceType":"datasetVersion","datasetId":6650630},{"sourceId":10727712,"sourceType":"datasetVersion","datasetId":6650652}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport nltk\nimport re \nimport os \nimport random \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom io import StringIO\nfrom nltk.corpus import stopwords\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nimport chardet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T02:36:28.295140Z","iopub.execute_input":"2025-02-12T02:36:28.295529Z","iopub.status.idle":"2025-02-12T02:37:03.892951Z","shell.execute_reply.started":"2025-02-12T02:36:28.295492Z","shell.execute_reply":"2025-02-12T02:37:03.891901Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"with open('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', 'rb') as f:\n    raw_data = f.read(100000)\n\nresult = chardet.detect(raw_data)\nprint(result['encoding'],', confidence: ', result['confidence'] )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:47:38.330255Z","iopub.execute_input":"2025-02-12T01:47:38.330554Z","iopub.status.idle":"2025-02-12T01:47:38.478780Z","shell.execute_reply.started":"2025-02-12T01:47:38.330526Z","shell.execute_reply":"2025-02-12T01:47:38.478072Z"}},"outputs":[{"name":"stdout","text":"utf-8 , confidence:  0.99\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"with open('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', \n          'r', encoding= 'utf-8', errors='replace') as f:\n    raw_text = f.read()  \n\ndf = pd.read_csv(StringIO(raw_text))\ndf.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:47:40.072540Z","iopub.execute_input":"2025-02-12T01:47:40.072854Z","iopub.status.idle":"2025-02-12T01:47:40.583735Z","shell.execute_reply.started":"2025-02-12T01:47:40.072828Z","shell.execute_reply":"2025-02-12T01:47:40.582869Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   UserName  ScreenName   Location     TweetAt  \\\n0      3799       48751     London  16-03-2020   \n1      3800       48752         UK  16-03-2020   \n2      3801       48753  Vagabonds  16-03-2020   \n3      3802       48754        NaN  16-03-2020   \n4      3803       48755        NaN  16-03-2020   \n\n                                       OriginalTweet           Sentiment  \n0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n1  advice Talk to your neighbours family to excha...            Positive  \n2  Coronavirus Australia: Woolworths to give elde...            Positive  \n3  My food stock is not the only one which is emp...            Positive  \n4  Me, ready to go at supermarket during the #COV...  Extremely Negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>advice Talk to your neighbours family to excha...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>Coronavirus Australia: Woolworths to give elde...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>My food stock is not the only one which is emp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>Me, ready to go at supermarket during the #COV...</td>\n      <td>Extremely Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"clean_df = df.copy()\nclean_df = clean_df[~clean_df.map(lambda x: isinstance(x, str) and '�' in x).any(axis=1)]\nplt.figure(figsize = (8, 4))\nsns.countplot(x = 'Sentiment',  data = clean_df, \n              order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'])\nplt.title(\"Sentimnent Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:47:42.715394Z","iopub.execute_input":"2025-02-12T01:47:42.715683Z","iopub.status.idle":"2025-02-12T01:47:43.033807Z","shell.execute_reply.started":"2025-02-12T01:47:42.715660Z","shell.execute_reply":"2025-02-12T01:47:43.032700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAssAAAGJCAYAAAB8cH4hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/2klEQVR4nO3de3zP9f//8fvbZgebbY6bZbZFOaUcY8qxMVJRSiIhIZGP5LSKqHzGcor8kj5lFB30iQqJHMNCKxFCmvhkh8LMIdtsz98ftdfXm700M/Zmt+vl8r5c9n6+nq/n6/E6vL3ue3m93nMYY4wAAAAAXKBEURcAAAAAuCrCMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgC4kF69eiksLKyoywAA/I2wDKDY2rFjhx588EGFhobKy8tLN9xwg9q0aaMZM2Zc0eUePnxYY8eO1bZt267oclzBsmXLNHbs2Eueb9GiRWrfvr3Kly8vDw8PBQcHq0uXLlq9enXhF1kAxWkfAsWdwxhjiroIALjaNm3apFatWqlKlSrq2bOngoKCdOjQIX3zzTfav3+/fv755yu27G+//VaNGjXSnDlz1KtXL6dpWVlZysnJkaen5xVb/tU0aNAgzZw5U/k91Rhj9PjjjysuLk716tXTgw8+qKCgICUlJWnRokVKSEjQxo0b1bRp0ytc+cVdbB8CuL64F3UBAFAUxo8fL39/f23dulUBAQFO01JTU4umKEklS5YssmW7gsmTJysuLk5DhgzRlClT5HA4rGnPP/+83n33Xbm7c+oCcPVwGwaAYmn//v2qXbv2BUFZkipWrHhB23vvvacGDRrI29tbZcuWVdeuXXXo0CGnPi1bttQtt9yiXbt2qVWrVipVqpRuuOEGxcbGWn3Wrl2rRo0aSZJ69+4th8Mhh8OhuLg4SRfes3zgwAE5HA5NmjRJM2fO1I033qhSpUqpbdu2OnTokIwxevnll1W5cmV5e3urY8eOOnr0qFNdYWFhuueee7Rhwwbdfvvt8vLy0o033qh58+ZdsJ5paWkaMmSIQkJC5OnpqWrVqmnixInKycnJs6bZs2eratWq8vT0VKNGjbR161arX69evTRz5kxJstbz3PB7vj///FMxMTGqUaOGJk2alGffHj166Pbbb7fe//LLL3rooYdUtmxZlSpVSk2aNNHSpUud5omLi5PD4dCBAwec2teuXSuHw6G1a9dabYWxDwFcX/j1HECxFBoaqvj4eP3444+65ZZbLtp3/PjxGj16tLp06aInnnhCv//+u2bMmKHmzZvr+++/dwrcx44dU7t27fTAAw+oS5cu+vjjjzVy5EjVqVNH7du3V82aNfXSSy9pzJgx6tevn5o1ayZJ/3hbwfz585WZmamnn35aR48eVWxsrLp06aLWrVtr7dq1GjlypH7++WfNmDFDw4YN0zvvvOM0/88//6wHH3xQffr0Uc+ePfXOO++oV69eatCggWrXri1JOn36tFq0aKHffvtN/fv3V5UqVbRp0yZFR0crKSlJ06ZNcxpzwYIFOnHihPr37y+Hw6HY2Fg98MAD+uWXX1SyZEn1799fhw8f1sqVK/Xuu+/+4z7ZsGGDjh49qiFDhsjNze0f+6ekpKhp06Y6ffq0Bg8erHLlymnu3Lm677779PHHH+v+++//xzHycqX2IYBrlAGAYmjFihXGzc3NuLm5mYiICDNixAjz5ZdfmszMTKd+Bw4cMG5ubmb8+PFO7Tt27DDu7u5O7S1atDCSzLx586y2jIwMExQUZDp37my1bd261Ugyc+bMuaCunj17mtDQUOt9YmKikWQqVKhg0tLSrPbo6Ggjydx2220mKyvLan/kkUeMh4eHOXPmjNUWGhpqJJn169dbbampqcbT09M8++yzVtvLL79sfHx8zN69e51qGjVqlHFzczMHDx50qqlcuXLm6NGjVr9PP/3USDKff/651TZw4ECT31PNa6+9ZiSZRYsW5av/kCFDjCTz9ddfW20nTpww4eHhJiwszGRnZxtjjJkzZ46RZBITE53mX7NmjZFk1qxZY7UVxj4EcH3hNgwAxVKbNm0UHx+v++67Tz/88INiY2MVFRWlG264QZ999pnV75NPPlFOTo66dOmiP/74w3oFBQXppptu0po1a5zG9fX11aOPPmq99/Dw0O23365ffvnlsup96KGH5O/vb71v3LixJOnRRx91uoe3cePGyszM1G+//eY0f61atawroJJUoUIFVa9e3amuhQsXqlmzZipTpozTukZGRio7O1vr1693GvPhhx9WmTJlrPe54xd0XdPT0yVJpUuXzlf/ZcuW6fbbb9edd95ptfn6+qpfv346cOCAdu3aVaA6rtQ+BHBt4jYMAMVWo0aN9MknnygzM1M//PCDFi1apKlTp+rBBx/Utm3bVKtWLe3bt0/GGN100015jnH+A3mVK1e+4F7bMmXKaPv27ZdVa5UqVZze5wbnkJCQPNuPHTt20flz6zq33759+7R9+3ZVqFAhzxrOf/Dx/DFzg/P5y84vPz8/SdKJEyfy1f/XX3+1fmk4V82aNa3p/3SLTV6u1D4EcG0iLAMo9jw8PNSoUSM1atRIN998s3r37q2FCxfqxRdfVE5OjhwOh7744os876P19fV1em93r625zG/ptBs3v8vLT7+cnBy1adNGI0aMyLPvzTffXKBl51eNGjUk/fX91506dSrQGHmxe6gwOzs7z/YrtQ8BXJsIywBwjoYNG0qSkpKSJElVq1aVMUbh4eEXhMWCutg3QhSlqlWr6uTJk4qMjCy0MS9lXe+8806VKVNG77//vp577rl/fMgvNDRUe/bsuaD9p59+sqZL/3fFOy0tzanfr7/+mu/azueq+xBA4eOeZQDF0po1a/K8Urhs2TJJUvXq1SVJDzzwgNzc3DRu3LgL+htjdOTIkUteto+Pj6QLw1tR69Kli+Lj4/Xll19eMC0tLU1nz5695DEvZV1LlSqlkSNHavfu3Ro5cmSe++e9997Tli1bJEl33323tmzZovj4eGv6qVOnNHv2bIWFhalWrVqS/volQJLTPdfZ2dmaPXv2Ja9PQdYLwLWNK8sAiqWnn35ap0+f1v33368aNWooMzNTmzZt0ocffqiwsDD17t1b0l9B65VXXlF0dLQOHDigTp06qXTp0kpMTNSiRYvUr18/DRs27JKWXbVqVQUEBGjWrFkqXbq0fHx81LhxY4WHh1+JVc234cOH67PPPtM999xjfa3cqVOntGPHDn388cc6cOCAypcvf0ljNmjQQJI0ePBgRUVFyc3NTV27dr1oDTt37tTkyZO1Zs0a6y/4JScna/HixdqyZYs2bdokSRo1apTef/99tW/fXoMHD1bZsmU1d+5cJSYm6r///a9KlPjrelDt2rXVpEkTRUdH6+jRoypbtqw++OCDAoX/XK66DwEUPsIygGJp0qRJWrhwoZYtW6bZs2crMzNTVapU0VNPPaUXXnjB6buTR40apZtvvllTp07VuHHjJP31YF3btm113333XfKyS5Ysqblz5yo6OlpPPvmkzp49qzlz5hR50CpVqpTWrVunf//731q4cKHmzZsnPz8/3XzzzRo3bpzTt3Hk1wMPPKCnn35aH3zwgd577z0ZYy4alkuUKKF58+apY8eOmj17tiZNmqT09HRVqFBBzZs3V2xsrCIiIiRJgYGB2rRpk0aOHKkZM2bozJkzuvXWW/X555+rQ4cOTuPOnz9f/fv314QJExQQEKA+ffqoVatWatOmzSWvk+S6+xBA4XMYnlgAAAAA8sQ9ywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg+9ZLiQ5OTk6fPiwSpcuzZ9BBQAAcEHGGJ04cULBwcHWHy76J4TlQnL48GGFhIQUdRkAAAD4B4cOHVLlypXz1ZewXEhKly4t6a+N7+fnV8TVAAAA4Hzp6ekKCQmxclt+EJYLSe6tF35+foRlAAAAF3Ypt8zygB8AAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADbci7oAAACuB+uatyjqEvC3FuvXFXUJuI5wZRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMBGkYbl9evX695771VwcLAcDocWL17sNN0YozFjxqhSpUry9vZWZGSk9u3b59Tn6NGj6t69u/z8/BQQEKA+ffro5MmTTn22b9+uZs2aycvLSyEhIYqNjb2gloULF6pGjRry8vJSnTp1tGzZskJfXwAAAFxbijQsnzp1SrfddptmzpyZ5/TY2FhNnz5ds2bN0ubNm+Xj46OoqCidOXPG6tO9e3ft3LlTK1eu1JIlS7R+/Xr169fPmp6enq62bdsqNDRUCQkJevXVVzV27FjNnj3b6rNp0yY98sgj6tOnj77//nt16tRJnTp10o8//njlVh4AAAAuz2GMMUVdhCQ5HA4tWrRInTp1kvTXVeXg4GA9++yzGjZsmCTp+PHjCgwMVFxcnLp27ardu3erVq1a2rp1qxo2bChJWr58ue6++27973//U3BwsN544w09//zzSk5OloeHhyRp1KhRWrx4sX766SdJ0sMPP6xTp05pyZIlVj1NmjRR3bp1NWvWrHzVn56eLn9/fx0/flx+fn6FtVkAANeIdc1bFHUJ+FuL9euKugS4qILkNZe9ZzkxMVHJycmKjIy02vz9/dW4cWPFx8dLkuLj4xUQEGAFZUmKjIxUiRIltHnzZqtP8+bNraAsSVFRUdqzZ4+OHTtm9Tl3Obl9cpeTl4yMDKWnpzu9AAAAcH1x2bCcnJwsSQoMDHRqDwwMtKYlJyerYsWKTtPd3d1VtmxZpz55jXHuMuz65E7PS0xMjPz9/a1XSEjIpa4iAAAAXJzLhmVXFx0drePHj1uvQ4cOFXVJAAAAKGTuRV2AnaCgIElSSkqKKlWqZLWnpKSobt26Vp/U1FSn+c6ePaujR49a8wcFBSklJcWpT+77f+qTOz0vnp6e8vT0LMCaoTg7+FKdoi4Bf6syZkdRlwAAuAa47JXl8PBwBQUFadWqVVZbenq6Nm/erIiICElSRESE0tLSlJCQYPVZvXq1cnJy1LhxY6vP+vXrlZWVZfVZuXKlqlevrjJlylh9zl1Obp/c5QAAAKB4KtKwfPLkSW3btk3btm2T9NdDfdu2bdPBgwflcDg0ZMgQvfLKK/rss8+0Y8cOPfbYYwoODra+MaNmzZpq166d+vbtqy1btmjjxo0aNGiQunbtquDgYElSt27d5OHhoT59+mjnzp368MMP9dprr2no0KFWHf/617+0fPlyTZ48WT/99JPGjh2rb7/9VoMGDbramwQAAAAupEhvw/j222/VqlUr631ugO3Zs6fi4uI0YsQInTp1Sv369VNaWpruvPNOLV++XF5eXtY88+fP16BBg3TXXXepRIkS6ty5s6ZPn25N9/f314oVKzRw4EA1aNBA5cuX15gxY5y+i7lp06ZasGCBXnjhBT333HO66aabtHjxYt1yyy1XYSsAAADAVbnM9yxf6/ieZeQH9yy7Du5ZRmHje5ZdB9+zDDvX1fcsAwAAAEWNsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA3CMgAAAGCDsAwAAADYICwDAAAANgjLAAAAgA2XDsvZ2dkaPXq0wsPD5e3trapVq+rll1+WMcbqY4zRmDFjVKlSJXl7eysyMlL79u1zGufo0aPq3r27/Pz8FBAQoD59+ujkyZNOfbZv365mzZrJy8tLISEhio2NvSrrCAAAANfl0mF54sSJeuONN/T6669r9+7dmjhxomJjYzVjxgyrT2xsrKZPn65Zs2Zp8+bN8vHxUVRUlM6cOWP16d69u3bu3KmVK1dqyZIlWr9+vfr162dNT09PV9u2bRUaGqqEhAS9+uqrGjt2rGbPnn1V1xcAAACuxb2oC7iYTZs2qWPHjurQoYMkKSwsTO+//762bNki6a+rytOmTdMLL7ygjh07SpLmzZunwMBALV68WF27dtXu3bu1fPlybd26VQ0bNpQkzZgxQ3fffbcmTZqk4OBgzZ8/X5mZmXrnnXfk4eGh2rVra9u2bZoyZYpTqAYAAEDx4tJXlps2bapVq1Zp7969kqQffvhBGzZsUPv27SVJiYmJSk5OVmRkpDWPv7+/GjdurPj4eElSfHy8AgICrKAsSZGRkSpRooQ2b95s9WnevLk8PDysPlFRUdqzZ4+OHTuWZ20ZGRlKT093egEAAOD64tJXlkeNGqX09HTVqFFDbm5uys7O1vjx49W9e3dJUnJysiQpMDDQab7AwEBrWnJysipWrOg03d3dXWXLlnXqEx4efsEYudPKlClzQW0xMTEaN25cIawlAAAAXJVLX1n+6KOPNH/+fC1YsEDfffed5s6dq0mTJmnu3LlFXZqio6N1/Phx63Xo0KGiLgkAAACFzKWvLA8fPlyjRo1S165dJUl16tTRr7/+qpiYGPXs2VNBQUGSpJSUFFWqVMmaLyUlRXXr1pUkBQUFKTU11Wncs2fP6ujRo9b8QUFBSklJceqT+z63z/k8PT3l6el5+SsJAAAAl+XSV5ZPnz6tEiWcS3Rzc1NOTo4kKTw8XEFBQVq1apU1PT09XZs3b1ZERIQkKSIiQmlpaUpISLD6rF69Wjk5OWrcuLHVZ/369crKyrL6rFy5UtWrV8/zFgwAAAAUDy4dlu+9916NHz9eS5cu1YEDB7Ro0SJNmTJF999/vyTJ4XBoyJAheuWVV/TZZ59px44deuyxxxQcHKxOnTpJkmrWrKl27dqpb9++2rJlizZu3KhBgwapa9euCg4OliR169ZNHh4e6tOnj3bu3KkPP/xQr732moYOHVpUqw4AAAAX4NK3YcyYMUOjR4/WU089pdTUVAUHB6t///4aM2aM1WfEiBE6deqU+vXrp7S0NN15551avny5vLy8rD7z58/XoEGDdNddd6lEiRLq3Lmzpk+fbk339/fXihUrNHDgQDVo0EDly5fXmDFj+No4AACAYs5hzv1zeCiw9PR0+fv76/jx4/Lz8yvqcuCiDr5Up6hLwN+qjNlR1CXgOrOueYuiLgF/a7F+XVGXABdVkLzm0rdhAAAAAEXJpW/DAIBr2R0z7ijqEvC3jU9vLOoSAFyjuLIMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYIywAAAIANwjIAAABgg7AMAAAA2CAsAwAAADYKFJZbt26ttLS0C9rT09PVunXry60JAAAAcAnuBZlp7dq1yszMvKD9zJkz+vrrry+7KAAAAFf2+rOfF3UJ+Nugyfde0fEvKSxv377d+nnXrl1KTk623mdnZ2v58uW64YYbCq86AAAAoAhdUliuW7euHA6HHA5HnrdbeHt7a8aMGYVWHAAAAFCULiksJyYmyhijG2+8UVu2bFGFChWsaR4eHqpYsaLc3NwKvUgAAACgKFxSWA4NDZUk5eTkXJFiAAAAAFdSoAf8JGnfvn1as2aNUlNTLwjPY8aMuezCAAAAgKJWoLD81ltvacCAASpfvryCgoLkcDisaQ6Hg7AMAACA60KBwvIrr7yi8ePHa+TIkYVdDwAAAOAyCvRHSY4dO6aHHnqosGsBAAAAXEqBwvJDDz2kFStWFHYtAAAAgEsp0G0Y1apV0+jRo/XNN9+oTp06KlmypNP0wYMHF0pxAAAAQFEqUFiePXu2fH19tW7dOq1bt85pmsPhICwDAADgulCg2zASExNtX7/88kuhFvjbb7/p0UcfVbly5eTt7a06dero22+/taYbYzRmzBhVqlRJ3t7eioyM1L59+5zGOHr0qLp37y4/Pz8FBASoT58+OnnypFOf7du3q1mzZvLy8lJISIhiY2MLdT0AAABw7SlQWL5ajh07pjvuuEMlS5bUF198oV27dmny5MkqU6aM1Sc2NlbTp0/XrFmztHnzZvn4+CgqKkpnzpyx+nTv3l07d+7UypUrtWTJEq1fv179+vWzpqenp6tt27YKDQ1VQkKCXn31VY0dO1azZ8++qusLAAAA11Kg2zAef/zxi05/5513ClTM+SZOnKiQkBDNmTPHagsPD7d+NsZo2rRpeuGFF9SxY0dJ0rx58xQYGKjFixera9eu2r17t5YvX66tW7eqYcOGkqQZM2bo7rvv1qRJkxQcHKz58+crMzNT77zzjjw8PFS7dm1t27ZNU6ZMcQrVAAAAKF4K/NVx575SU1O1evVqffLJJ0pLSyu04j777DM1bNhQDz30kCpWrKh69erprbfesqYnJiYqOTlZkZGRVpu/v78aN26s+Ph4SVJ8fLwCAgKsoCxJkZGRKlGihDZv3mz1ad68uTw8PKw+UVFR2rNnj44dO5ZnbRkZGUpPT3d6AQAA4PpSoCvLixYtuqAtJydHAwYMUNWqVS+7qFy//PKL3njjDQ0dOlTPPfectm7dqsGDB8vDw0M9e/ZUcnKyJCkwMNBpvsDAQGtacnKyKlas6DTd3d1dZcuWdepz7hXrc8dMTk52uu0jV0xMjMaNG1c4KwoAAACXVGj3LJcoUUJDhw7V1KlTC2tI5eTkqH79+vr3v/+tevXqqV+/furbt69mzZpVaMsoqOjoaB0/ftx6HTp0qKhLAgAAQCEr1Af89u/fr7NnzxbaeJUqVVKtWrWc2mrWrKmDBw9KkoKCgiRJKSkpTn1SUlKsaUFBQUpNTXWafvbsWR09etSpT15jnLuM83l6esrPz8/pBQAAgOtLgW7DGDp0qNN7Y4ySkpK0dOlS9ezZs1AKk6Q77rhDe/bscWrbu3evQkNDJf31sF9QUJBWrVqlunXrSvrrmy02b96sAQMGSJIiIiKUlpamhIQENWjQQJK0evVq5eTkqHHjxlaf559/XllZWdYfWFm5cqWqV6+e5y0YAAAAKB4KFJa///57p/clSpRQhQoVNHny5H/8poxL8cwzz6hp06b697//rS5dumjLli2aPXu29ZVuDodDQ4YM0SuvvKKbbrpJ4eHhGj16tIKDg9WpUydJf12JbteunXX7RlZWlgYNGqSuXbsqODhYktStWzeNGzdOffr00ciRI/Xjjz/qtddeK9RbSgAAAHDtKVBYXrNmTWHXkadGjRpp0aJFio6O1ksvvaTw8HBNmzZN3bt3t/qMGDFCp06dUr9+/ZSWlqY777xTy5cvl5eXl9Vn/vz5GjRokO666y6VKFFCnTt31vTp063p/v7+WrFihQYOHKgGDRqofPnyGjNmDF8bBwAAUMwVKCzn+v33363bJKpXr64KFSoUSlHnuueee3TPPffYTnc4HHrppZf00ksv2fYpW7asFixYcNHl3Hrrrfr6668LXCcAAACuPwV6wO/UqVN6/PHHValSJTVv3lzNmzdXcHCw+vTpo9OnTxd2jQAAAECRKFBYHjp0qNatW6fPP/9caWlpSktL06effqp169bp2WefLewaAQAAgCJRoNsw/vvf/+rjjz9Wy5Ytrba7775b3t7e6tKli954443Cqg8AAAAoMgW6snz69OkL/mqeJFWsWJHbMAAAAHDdKFBYjoiI0IsvvqgzZ85YbX/++afGjRuniIiIQisOAAAAKEoFug1j2rRpateunSpXrqzbbrtNkvTDDz/I09NTK1asKNQCAQAAgKJSoLBcp04d7du3T/Pnz9dPP/0kSXrkkUfUvXt3eXt7F2qB17sGw+cVdQn4W8KrjxV1CQAAwMUUKCzHxMQoMDBQffv2dWp/55139Pvvv2vkyJGFUhwAAABQlAp0z/Kbb76pGjVqXNBeu3ZtzZo167KLAgAAAFxBgcJycnKyKlWqdEF7hQoVlJSUdNlFAQAAAK6gQGE5JCREGzduvKB948aNCg4OvuyiAAAAAFdQoHuW+/btqyFDhigrK0utW7eWJK1atUojRozgL/gBAADgulGgsDx8+HAdOXJETz31lDIzMyVJXl5eGjlypKKjowu1QAAAAKCoFCgsOxwOTZw4UaNHj9bu3bvl7e2tm266SZ6enoVdHwAAAFBkChSWc/n6+qpRo0aFVQsAAADgUgr0gB8AAABQHBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbFxTYXnChAlyOBwaMmSI1XbmzBkNHDhQ5cqVk6+vrzp37qyUlBSn+Q4ePKgOHTqoVKlSqlixooYPH66zZ8869Vm7dq3q168vT09PVatWTXFxcVdhjQAAAODKrpmwvHXrVr355pu69dZbndqfeeYZff7551q4cKHWrVunw4cP64EHHrCmZ2dnq0OHDsrMzNSmTZs0d+5cxcXFacyYMVafxMREdejQQa1atdK2bds0ZMgQPfHEE/ryyy+v2voBAADA9VwTYfnkyZPq3r273nrrLZUpU8ZqP378uN5++21NmTJFrVu3VoMGDTRnzhxt2rRJ33zzjSRpxYoV2rVrl9577z3VrVtX7du318svv6yZM2cqMzNTkjRr1iyFh4dr8uTJqlmzpgYNGqQHH3xQU6dOLZL1BQAAgGu4JsLywIED1aFDB0VGRjq1JyQkKCsry6m9Ro0aqlKliuLj4yVJ8fHxqlOnjgIDA60+UVFRSk9P186dO60+548dFRVljZGXjIwMpaenO70AAABwfXEv6gL+yQcffKDvvvtOW7duvWBacnKyPDw8FBAQ4NQeGBio5ORkq8+5QTl3eu60i/VJT0/Xn3/+KW9v7wuWHRMTo3HjxhV4vQAAAOD6XPrK8qFDh/Svf/1L8+fPl5eXV1GX4yQ6OlrHjx+3XocOHSrqkgAAAFDIXDosJyQkKDU1VfXr15e7u7vc3d21bt06TZ8+Xe7u7goMDFRmZqbS0tKc5ktJSVFQUJAkKSgo6IJvx8h9/099/Pz88ryqLEmenp7y8/NzegEAAOD64tJh+a677tKOHTu0bds269WwYUN1797d+rlkyZJatWqVNc+ePXt08OBBRURESJIiIiK0Y8cOpaamWn1WrlwpPz8/1apVy+pz7hi5fXLHAAAAQPHk0vcsly5dWrfccotTm4+Pj8qVK2e19+nTR0OHDlXZsmXl5+enp59+WhEREWrSpIkkqW3btqpVq5Z69Oih2NhYJScn64UXXtDAgQPl6ekpSXryySf1+uuva8SIEXr88ce1evVqffTRR1q6dOnVXWEAAAC4FJcOy/kxdepUlShRQp07d1ZGRoaioqL0//7f/7Omu7m5acmSJRowYIAiIiLk4+Ojnj176qWXXrL6hIeHa+nSpXrmmWf02muvqXLlyvrPf/6jqKioolglAAAAuIhrLiyvXbvW6b2Xl5dmzpypmTNn2s4TGhqqZcuWXXTcli1b6vvvvy+MEgEAAHCdcOl7lgEAAICiRFgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMCGS4flmJgYNWrUSKVLl1bFihXVqVMn7dmzx6nPmTNnNHDgQJUrV06+vr7q3LmzUlJSnPocPHhQHTp0UKlSpVSxYkUNHz5cZ8+edeqzdu1a1a9fX56enqpWrZri4uKu9OoBAADAxbl0WF63bp0GDhyob775RitXrlRWVpbatm2rU6dOWX2eeeYZff7551q4cKHWrVunw4cP64EHHrCmZ2dnq0OHDsrMzNSmTZs0d+5cxcXFacyYMVafxMREdejQQa1atdK2bds0ZMgQPfHEE/ryyy+v6voCAADAtbgXdQEXs3z5cqf3cXFxqlixohISEtS8eXMdP35cb7/9thYsWKDWrVtLkubMmaOaNWvqm2++UZMmTbRixQrt2rVLX331lQIDA1W3bl29/PLLGjlypMaOHSsPDw/NmjVL4eHhmjx5siSpZs2a2rBhg6ZOnaqoqKirvt4AAABwDS59Zfl8x48flySVLVtWkpSQkKCsrCxFRkZafWrUqKEqVaooPj5ekhQfH686deooMDDQ6hMVFaX09HTt3LnT6nPuGLl9csfIS0ZGhtLT051eAAAAuL5cM2E5JydHQ4YM0R133KFbbrlFkpScnCwPDw8FBAQ49Q0MDFRycrLV59ygnDs9d9rF+qSnp+vPP//Ms56YmBj5+/tbr5CQkMteRwAAALiWayYsDxw4UD/++KM++OCDoi5FkhQdHa3jx49br0OHDhV1SQAAAChkLn3Pcq5BgwZpyZIlWr9+vSpXrmy1BwUFKTMzU2lpaU5Xl1NSUhQUFGT12bJli9N4ud+WcW6f879BIyUlRX5+fvL29s6zJk9PT3l6el72ugEAAMB1ufSVZWOMBg0apEWLFmn16tUKDw93mt6gQQOVLFlSq1atstr27NmjgwcPKiIiQpIUERGhHTt2KDU11eqzcuVK+fn5qVatWlafc8fI7ZM7BgAAAIonl76yPHDgQC1YsECffvqpSpcubd1j7O/vL29vb/n7+6tPnz4aOnSoypYtKz8/Pz399NOKiIhQkyZNJElt27ZVrVq11KNHD8XGxio5OVkvvPCCBg4caF0ZfvLJJ/X6669rxIgRevzxx7V69Wp99NFHWrp0aZGtOwAAAIqeS19ZfuONN3T8+HG1bNlSlSpVsl4ffvih1Wfq1Km655571LlzZzVv3lxBQUH65JNPrOlubm5asmSJ3NzcFBERoUcffVSPPfaYXnrpJatPeHi4li5dqpUrV+q2227T5MmT9Z///IevjQMAACjmXPrKsjHmH/t4eXlp5syZmjlzpm2f0NBQLVu27KLjtGzZUt9///0l1wgAAIDrl0tfWQYAAACKEmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmEZAAAAsEFYBgAAAGwQlgEAAAAbhGUAAADABmH5PDNnzlRYWJi8vLzUuHFjbdmypahLAgAAQBEhLJ/jww8/1NChQ/Xiiy/qu+++02233aaoqCilpqYWdWkAAAAoAoTlc0yZMkV9+/ZV7969VatWLc2aNUulSpXSO++8U9SlAQAAoAi4F3UBriIzM1MJCQmKjo622kqUKKHIyEjFx8df0D8jI0MZGRnW++PHj0uS0tPTL2m52Rl/FrBiFLZL3XcFceJM9hVfBvLnauzvs3+eveLLQP5cjf196iz721Vcjf39Z8bpK74M5M+l7O/cvsaYfM9DWP7bH3/8oezsbAUGBjq1BwYG6qeffrqgf0xMjMaNG3dBe0hIyBWrEVeW/4wni7oEXE0x/kVdAa4i/5Hs72LFn/1dnIyYeenznDhxQv75PE4IywUUHR2toUOHWu9zcnJ09OhRlStXTg6Howgru7rS09MVEhKiQ4cOyc/Pr6jLwRXG/i5e2N/FC/u7eCmu+9sYoxMnTig4ODjf8xCW/1a+fHm5ubkpJSXFqT0lJUVBQUEX9Pf09JSnp6dTW0BAwJUs0aX5+fkVqw9bccf+Ll7Y38UL+7t4KY77O79XlHPxgN/fPDw81KBBA61atcpqy8nJ0apVqxQREVGElQEAAKCocGX5HEOHDlXPnj3VsGFD3X777Zo2bZpOnTql3r17F3VpAAAAKAKE5XM8/PDD+v333zVmzBglJyerbt26Wr58+QUP/eH/eHp66sUXX7zglhRcn9jfxQv7u3hhfxcv7O/8c5hL+e4MAAAAoBjhnmUAAADABmEZAAAAsEFYBgAAAGwQlmFr7dq1cjgcSktLK+pSrriwsDBNmzatqMu47rBdYac4/fvi6vK7L/g8X9tc/TPnyschYbkAevXqJYfDccGrXbt2+R6jZcuWGjJkyJUr0kXkbqsJEyY4tS9evLhI/tJhXFxcnn88ZuvWrerXr99Vr+dyuNK2vZ6267Xmah0HBw4ckMPh0LZt2wptTFyac889Hh4eqlatml566SWdPXv2ssZt2rSpkpKSrD/UUNw+z5zT8+9KHYOSax+HhOUCateunZKSkpxe77//fqEuwxhTKAdgUfPy8tLEiRN17Nixoi7FVoUKFVSqVKmiLuOSufq2vVa367XGlY6DzMzMoi7hupZ77tm3b5+effZZjR07Vq+++upljenh4aGgoKB//OXqev48c07PvytxDEqufRwSlgvI09NTQUFBTq8yZcpI+uu/Ejw8PPT1119b/WNjY1WxYkWlpKSoV69eWrdunV577TXrN7QDBw5Y/wXxxRdfqEGDBvL09NSGDRuUk5OjmJgYhYeHy9vbW7fddps+/vhja+zc+b788kvVq1dP3t7eat26tVJTU/XFF1+oZs2a8vPzU7du3XT69Glrvn8a91ynTp2Sn5/fBdMXL14sHx8fnThxwnZbRUZGKigoSDExMRfdphs2bFCzZs3k7e2tkJAQDR48WKdOnbKmJyUlqUOHDvL29lZ4eLgWLFhwwX/HTJkyRXXq1JGPj49CQkL01FNP6eTJk9Z26t27t44fP25t97Fjx0py/m+dbt266eGHH3aqLSsrS+XLl9e8efMuedtdSfnZtmzX619hHAcOh0OLFy92micgIEBxcXGSpPDwcElSvXr15HA41LJlS0l/XWnq1KmTxo8fr+DgYFWvXl2S9O6776phw4YqXbq0goKC1K1bN6WmphbeShdTueee0NBQDRgwQJGRkfrss8907NgxPfbYYypTpoxKlSql9u3ba9++fdZ8v/76q+69916VKVNGPj4+ql27tpYtWybJ+b+/i+vnmXP6X/JzTrc7BiVdv8ehwSXr2bOn6dix40X7DB8+3ISGhpq0tDTz3XffGQ8PD/Ppp58aY4xJS0szERERpm/fviYpKckkJSWZs2fPmjVr1hhJ5tZbbzUrVqwwP//8szly5Ih55ZVXTI0aNczy5cvN/v37zZw5c4ynp6dZu3atMcZY8zVp0sRs2LDBfPfdd6ZatWqmRYsWpm3btua7774z69evN+XKlTMTJkywaszvuMeOHTPGGNO3b19z9913O63nfffdZx577LF/3FaffPKJ8fLyMocOHTLGGLNo0SJz7uH3888/Gx8fHzN16lSzd+9es3HjRlOvXj3Tq1cvq09kZKSpW7eu+eabb0xCQoJp0aKF8fb2NlOnTrX6TJ061axevdokJiaaVatWmerVq5sBAwYYY4zJyMgw06ZNM35+ftZ2P3HihDHGmNDQUGucJUuWGG9vb2uaMcZ8/vnnxtvb26Snp+dr210N+dm2bNfrX2EdB5LMokWLnMb29/c3c+bMMcYYs2XLFiPJfPXVVyYpKckcOXLEWr6vr6/p0aOH+fHHH82PP/5ojDHm7bffNsuWLTP79+838fHxJiIiwrRv394a+/x/X/DP8jr33HfffaZ+/frmvvvuMzVr1jTr168327ZtM1FRUaZatWomMzPTGGNMhw4dTJs2bcz27dvN/v37zeeff27WrVtnjHHeF8Xx88w5/f/k95x+/jz169e3fr4ej0PCcgH07NnTuLm5GR8fH6fX+PHjrT4ZGRmmbt26pkuXLqZWrVqmb9++TmO0aNHC/Otf/3Jqyz1QFi9ebLWdOXPGlCpVymzatMmpb58+fcwjjzziNN9XX31lTY+JiTGSzP79+622/v37m6ioqEseN/eDtXnzZuPm5mYOHz5sjDEmJSXFuLu7X/SAO/eD1aRJE/P4448bYy4My3369DH9+vVzmvfrr782JUqUMH/++afZvXu3kWS2bt1qTd+3b5+R5BTqzrdw4UJTrlw56/2cOXOMv7//Bf3O/fBlZWWZ8uXLm3nz5lnTH3nkEfPwww8bY/K37a6G/Gxbtuv1rzCOA2P+OSwnJiYaSeb777+/YPmBgYEmIyPjonVu3brVSLJOaoTlS3fuvs7JyTErV640np6eplOnTkaS2bhxo9X3jz/+MN7e3uajjz4yxhhTp04dM3bs2DzHPX9fFLfPM+f0gp3Tzz0Ghw0bZvbu3XvdHof8uesCatWqld544w2ntrJly1o/e3h4aP78+br11lsVGhqqqVOn5nvshg0bWj///PPPOn36tNq0aePUJzMzU/Xq1XNqu/XWW62fAwMDVapUKd14441ObVu2bLnkcXPdfvvtql27tubOnatRo0bpvffeU2hoqJo3b56v9Zo4caJat26tYcOGXTDthx9+0Pbt2zV//nyrzRijnJwcJSYmau/evXJ3d1f9+vWt6dWqVbP+myzXV199pZiYGP30009KT0/X2bNndebMGZ0+fTrf9zi5u7urS5cumj9/vnr06KFTp07p008/1QcffCCpYNvuSrPbtmzX4qWgx0HNmjUva7l16tSRh4eHU1tCQoLGjh2rH374QceOHVNOTo4k6eDBg6pVq9ZlLa84W7JkiXx9fZWVlaWcnBx169ZNDzzwgJYsWaLGjRtb/cqVK6fq1atr9+7dkqTBgwdrwIABWrFihSIjI9W5c2enc8alut4+z5zT839Oz+sYHDt2rFatWiV3d/fr8jgkLBeQj4+PqlWrdtE+mzZtkiQdPXpUR48elY+PT77HzpV7X+jSpUt1ww03OPU7/++5lyxZ0vrZ4XA4vc9tyz1hXcq453riiSc0c+ZMjRo1SnPmzFHv3r3z/cR98+bNFRUVpejoaPXq1ctp2smTJ9W/f38NHjz4gvmqVKmivXv3/uP4Bw4c0D333KMBAwZo/PjxKlu2rDZs2KA+ffooMzPzkh4I6N69u1q0aKHU1FStXLlS3t7e1pPRBd12V5LdtmW7Fi8FPQ6kv/59MMY4TcvKysrXcs//t+3UqVOKiopSVFSU5s+frwoVKujgwYOKioriAcDLlBvqPDw8FBwcLHd3d+t+0Yt54oknFBUVpaVLl2rFihWKiYnR5MmT9fTTTxe4luvp88w5Pf/n9LyOwfy6Vo9DwvIVsn//fj3zzDN666239OGHH6pnz5766quvVKLEX89Uenh4KDs7+x/HqVWrljw9PXXw4EG1aNGi0Oor6LiPPvqoRowYoenTp2vXrl3q2bPnJS13woQJqlu3rvUQUK769etr165dtv9YVa9eXWfPntX333+vBg0aSPrrN8Zzn/5PSEhQTk6OJk+ebG3njz76yGmc/G73pk2bKiQkRB9++KG++OILPfTQQ9Y/VFdqn1yuvLYt27X4KchxIP31hHlSUpL1ft++fU4PD+VeOc7Pfv7pp5905MgRTZgwQSEhIZKkb7/99pLXBRfKK9TVrFlTZ8+e1ebNm9W0aVNJ0pEjR7Rnzx6nq/ghISF68skn9eSTTyo6OlpvvfVWniGFz/OFOKf/H7tfLK7n45CwXEAZGRlKTk52anN3d1f58uWVnZ2tRx99VFFRUerdu7fatWunOnXqaPLkyRo+fLikv57m3Lx5sw4cOCBfX1+n/+45V+nSpTVs2DA988wzysnJ0Z133qnjx49r48aN8vPzu+SwernjlilTRg888ICGDx+utm3bqnLlype03Dp16qh79+6aPn26U/vIkSPVpEkTDRo0SE888YR8fHy0a9curVy5Uq+//rpq1KihyMhI9evXT2+88YZKliypZ599Vt7e3tZvwdWqVVNWVpZmzJihe++9Vxs3btSsWbOclhMWFqaTJ09q1apVuu2221SqVCnbK6PdunXTrFmztHfvXq1Zs+ayt92Vlte2ZbsWPwU5DiSpdevWev311xUREaHs7GyNHDnS6UpWxYoV5e3treXLl6ty5cry8vKyvg/1fFWqVJGHh4dmzJihJ598Uj/++KNefvnlK7vixdhNN92kjh07qm/fvnrzzTdVunRpjRo1SjfccIM6duwoSRoyZIjat2+vm2++WceOHdOaNWtsb78pjp9nzukFO6ef67o+DvN9dzMsPXv2NJIueFWvXt0YY8y4ceNMpUqVzB9//GHN89///td4eHiYbdu2GWOM2bNnj2nSpInx9vY2kkxiYqLtAy85OTlm2rRppnr16qZkyZKmQoUKJioqKs8nSHPldWP8iy++aG677bbLGtcYY1atWmUkWTfs/9O2Ov/J2cTEROPh4WHOP/y2bNli2rRpY3x9fY2Pj4+59dZbnR6wOHz4sGnfvr3x9PQ0oaGhZsGCBaZixYpm1qxZVp8pU6aYSpUqGW9vbxMVFWXmzZt3wTo8+eSTply5ckaSefHFF40xzg8M5Nq1a5eRZEJDQ01OTo7TtH/adldDfrct2/X6VljHwW+//Wbatm1rfHx8zE033WSWLVvm9ICfMca89dZbJiQkxJQoUcK0aNHCdvnGGLNgwQITFhZmPD09TUREhPnss8+cHhDkAb9Ld7FvbTh69Kjp0aOH8ff3tz6ne/futaYPGjTIVK1a1Xh6epoKFSqYHj16WOeovPZFcfo8c06/vHP6ua7X49BhzHk3qQH/4N1339Uzzzyjw4cPX/BQz9X0v//9TyEhIfrqq6901113FVkd1xu2KwAUH65yTndl3IaBfDt9+rSSkpI0YcIE9e/f/6p/qFavXq2TJ0+qTp06SkpK0ogRIxQWFpbvb+NA3tiuAFD8FPU5/VrCX/BDvsXGxqpGjRoKCgpSdHT0VV9+VlaWnnvuOdWuXVv333+/KlSooLVr117whDAuDdsVAIqfoj6nX0u4DQMAAACwwZVlAAAAwAZhGQAAALBBWAYAAABsEJYBAAAAG4RlAAAAwAZhGQBgWbt2rRwOh9LS0oq6FABwCYRlAHBBv//+uwYMGKAqVarI09NTQUFBioqK0saNGwttGS1bttSQIUOc2po2baqkpCT5+/sX2nIKqlevXurUqVNRlwGgmOMv+AGAC+rcubMyMzM1d+5c3XjjjUpJSdGqVat05MiRK7pcDw8PBQUFXdFlAMC1hCvLAOBi0tLS9PXXX2vixIlq1aqVQkNDdfvttys6Olr33Xef1eeJJ55QhQoV5Ofnp9atW+uHH36wxhg7dqzq1q2rd999V2FhYfL391fXrl114sQJSX9dtV23bp1ee+01ORwOORwOHThw4ILbMOLi4hQQEKAlS5aoevXqKlWqlB588EGdPn1ac+fOVVhYmMqUKaPBgwcrOzvbWn5GRoaGDRumG264QT4+PmrcuLHWrl1rTc8d98svv1TNmjXl6+urdu3aKSkpyap/7ty5+vTTT636zp0fAK4WwjIAuBhfX1/5+vpq8eLFysjIyLPPQw89pNTUVH3xxRdKSEhQ/fr1ddddd+no0aNWn/3792vx4sVasmSJlixZonXr1mnChAmSpNdee00RERHq27evkpKSlJSUpJCQkDyXdfr0aU2fPl0ffPCBli9frrVr1+r+++/XsmXLtGzZMr377rt688039fHHH1vzDBo0SPHx8frggw+0fft2PfTQQ2rXrp327dvnNO6kSZP07rvvav369Tp48KCGDRsmSRo2bJi6dOliBeikpCQ1bdr0srctAFwqwjIAuBh3d3fFxcVp7ty5CggI0B133KHnnntO27dvlyRt2LBBW7Zs0cKFC9WwYUPddNNNmjRpkgICApwCa05OjuLi4nTLLbeoWbNm6tGjh1atWiVJ8vf3l4eHh0qVKqWgoCAFBQXJzc0tz3qysrL0xhtvqF69emrevLkefPBBbdiwQW+//bZq1aqle+65R61atdKaNWskSQcPHtScOXO0cOFCNWvWTFWrVtWwYcN05513as6cOU7jzpo1Sw0bNlT9+vU1aNAgqz5fX195e3tb92sHBQXJw8PjimxvALgY7lkGABfUuXNndejQQV9//bW++eYbffHFF4qNjdV//vMfnTp1SidPnlS5cuWc5vnzzz+1f/9+631YWJhKly5tva9UqZJSU1MvuZZSpUqpatWq1vvAwECFhYXJ19fXqS137B07dig7O1s333yz0zgZGRlONZ8/bkHrA4AribAMAC7Ky8tLbdq0UZs2bTR69Gg98cQTevHFF/XUU0+pUqVKed7DGxAQYP1csmRJp2kOh0M5OTmXXEde41xs7JMnT8rNzU0JCQkXXK0+N2DnNYYx5pLrA4AribAMANeIWrVqafHixapfv76Sk5Pl7u6usLCwAo/n4eHh9FBeYalXr56ys7OVmpqqZs2aFXicK1UfAFwK7lkGABdz5MgRtW7dWu+99562b9+uxMRELVy4ULGxserYsaMiIyMVERGhTp06acWKFTpw4IA2bdqk559/Xt9++22+lxMWFqbNmzfrwIED+uOPPwp01TkvN998s7p3767HHntMn3zyiRITE7VlyxbFxMRo6dKll1Tf9u3btWfPHv3xxx/KysoqlPoA4FIQlgHAxfj6+qpx48aaOnWqmjdvrltuuUWjR49W37599frrr8vhcGjZsmVq3ry5evfurZtvvlldu3bVr7/+qsDAwHwvZ9iwYXJzc1OtWrVUoUIFHTx4sNDWYc6cOXrsscf07LPPqnr16urUqZO2bt2qKlWq5HuMvn37qnr16mrYsKEqVKhQqH+QBQDyy2G4QQwAAADIE1eWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACwQVgGAAAAbBCWAQAAABuEZQAAAMAGYRkAAACw8f8BmTvaNGx5WZcAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"<font size = \"3\">\n긍정이 조금 더 많긴 하나, 대칭적인 분포를 보인다.\n\n-----\n\nNLP의 전처리 단계는\n\n1. Stop word (불용어) 제거\n2. 영어의 경우 소문자화 lowercase 해주기\n3. stemming을 통해 어원 추출\n4. Tokenization을 통해 숫자로 바꿔주기","metadata":{}},{"cell_type":"code","source":"def convert_sentiment(x):\n    if x == 'Extremely Positive':\n        x = 2\n    elif x == 'Positive':\n        x = 2\n    elif x == 'Neutral':\n        x = 1\n    elif x == 'Negative':\n        x = 0\n    else:\n        x = 0\n    return x\n\n\ndef preprocess_text(x):\n    x = re.sub(r'@\\S*', \"\", x)\n    x = re.sub(r'https?:\\/\\/\\S*', \"\", x)\n    x = re.sub(r'#\\S*', \"\", x)\n    x = re.sub('\\n', \"\", x)\n    x = re.sub('', \"'\", x)\n    x = x.lower()\n    return x.strip()\n\n\npreprocess_df = clean_df.copy()\npreprocess_df['Sentiment'] = preprocess_df['Sentiment'].apply(convert_sentiment)\npreprocess_df['OriginalTweet'] = preprocess_df['OriginalTweet'].apply(preprocess_text)\npreprocess_df.drop(columns = ['UserName', 'ScreenName','Location', 'TweetAt'], inplace = True)\npreprocess_df = preprocess_df[preprocess_df['OriginalTweet'].str.split().str.len() > 5]\npreprocess_df.reset_index(drop = True, inplace = True)\n\n\npreprocess_df['OriginalTweet'].str.len().describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:47:47.034852Z","iopub.execute_input":"2025-02-12T01:47:47.035160Z","iopub.status.idle":"2025-02-12T01:47:47.521698Z","shell.execute_reply.started":"2025-02-12T01:47:47.035108Z","shell.execute_reply":"2025-02-12T01:47:47.520854Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"count    40289.000000\nmean       161.741120\nstd         65.986932\nmin         19.000000\n25%        105.000000\n50%        167.000000\n75%        218.000000\nmax        306.000000\nName: OriginalTweet, dtype: float64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"preprocess_df.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:47:59.399230Z","iopub.execute_input":"2025-02-12T01:47:59.399544Z","iopub.status.idle":"2025-02-12T01:47:59.408736Z","shell.execute_reply.started":"2025-02-12T01:47:59.399522Z","shell.execute_reply":"2025-02-12T01:47:59.408005Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                           OriginalTweet  Sentiment\n12779  wake up to a new realityhappy to get eggs and ...          0\n32070  but why bail out juniors that are already fail...          0\n4800   it's real shame that the so called \"hindustan\"...          0\n5584   the church of jesus christ of latter-day saint...          1\n16828  i m bidding up prices on other states   says r...          2\n28433  ?free review and immediate budget??the best fi...          2\n18701  golden boys proved to be the most useless grou...          0\n36158              do you really need that toilet paper?          1\n11836  covid 19 x climate crisis food insecurity thre...          0\n17062  price gouging in a state of emergency is illeg...          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12779</th>\n      <td>wake up to a new realityhappy to get eggs and ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32070</th>\n      <td>but why bail out juniors that are already fail...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4800</th>\n      <td>it's real shame that the so called \"hindustan\"...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5584</th>\n      <td>the church of jesus christ of latter-day saint...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16828</th>\n      <td>i m bidding up prices on other states   says r...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>28433</th>\n      <td>?free review and immediate budget??the best fi...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18701</th>\n      <td>golden boys proved to be the most useless grou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36158</th>\n      <td>do you really need that toilet paper?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11836</th>\n      <td>covid 19 x climate crisis food insecurity thre...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17062</th>\n      <td>price gouging in a state of emergency is illeg...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, tokens, labels):\n        self.tokens = {key: torch.tensor(val) for key, val in tokens.items()}\n        self.labels = torch.LongTensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.tokens.items()}\n        item['labels'] = self.labels[idx]\n        return item\n\n\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    preprocess_df['OriginalTweet'].tolist(), preprocess_df['Sentiment'].tolist(),\n    test_size = 0.2, random_state = 42\n)\n\n\n\nbase_model = \"bert-base-uncased\"\nbert_tokenizer = BertTokenizer.from_pretrained(base_model)\ntrain_tokenized = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=184)\nval_tokenized = bert_tokenizer(val_texts, truncation=True, padding=True, max_length=184)\n\n\ntrain_dataset = TweetDataset(train_tokenized, train_labels)\nval_dataset = TweetDataset(val_tokenized, val_labels)\n\n\ndevice=  torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nbert_model = BertForSequenceClassification.from_pretrained(base_model, num_labels = 3)\nbert_model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:58:40.844604Z","iopub.execute_input":"2025-02-12T01:58:40.844924Z","iopub.status.idle":"2025-02-12T01:59:16.339487Z","shell.execute_reply.started":"2025-02-12T01:58:40.844901Z","shell.execute_reply":"2025-02-12T01:59:16.338708Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4cb223a70e4302bebde0f72d33ca50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df022ce66d584086b2edb4b595995986"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee28de7976d44bb198b6151c7519ab62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a8a7a4c6734d54ae2051640a387f8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"870986b339534b248f8f534bf5b116a3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = \"./results\",\n    num_train_epochs = 3,\n    per_device_train_batch_size = 32,\n    per_device_eval_batch_size = 32,\n    warmup_steps = 500,\n    weight_decay = 0.01,\n    logging_dir = './logs',\n    logging_steps = 200,\n    eval_strategy = 'epoch',\n    save_strategy = 'epoch',\n    report_to = \"none\",\n    fp16 = True,\n)\n\ntrainer = Trainer(\n    model = bert_model,\n    args= training_args,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:59:19.402889Z","iopub.execute_input":"2025-02-12T01:59:19.403199Z","iopub.status.idle":"2025-02-12T02:28:42.841992Z","shell.execute_reply.started":"2025-02-12T01:59:19.403175Z","shell.execute_reply":"2025-02-12T02:28:42.840979Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1512' max='1512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1512/1512 29:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.566100</td>\n      <td>0.376634</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.237700</td>\n      <td>0.241339</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.133600</td>\n      <td>0.230574</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1512, training_loss=0.3631836081308032, metrics={'train_runtime': 1762.8956, 'train_samples_per_second': 54.849, 'train_steps_per_second': 0.858, 'total_flos': 9142940486071728.0, 'train_loss': 0.3631836081308032, 'epoch': 3.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def predict_sentiment(text):\n    inputs = bert_tokenizer(text, return_tensors = \"pt\", truncation = True, padding = True,\n                           max_length = 184)\n    inputs = {key : val.to(device)  for key, val in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim= 1).item()\n    \n    return predicted_class\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T02:29:20.701742Z","iopub.execute_input":"2025-02-12T02:29:20.702036Z","iopub.status.idle":"2025-02-12T02:29:20.706611Z","shell.execute_reply.started":"2025-02-12T02:29:20.702014Z","shell.execute_reply":"2025-02-12T02:29:20.705751Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"with open('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv',\n          'r', encoding ='utf-8' , errors = 'replace') as f:\n    raw_data = f.read()\n\n\ntest_df = pd.read_csv(StringIO(raw_data))\n\n\n\n\n    \nclean_df = test_df.copy()\nclean_df = clean_df[~clean_df.map(lambda x: isinstance(x, str) and '�' in x).any(axis=1)]\npreprocess_df = clean_df.copy()\npreprocess_df['Sentiment'] = preprocess_df['Sentiment'].apply(convert_sentiment)\npreprocess_df['OriginalTweet'] = preprocess_df['OriginalTweet'].apply(preprocess_text)\npreprocess_df.drop(columns = ['UserName', 'ScreenName','Location', 'TweetAt'], inplace = True)\npreprocess_df = preprocess_df[preprocess_df['OriginalTweet'].str.split().str.len() > 5]\npreprocess_df.reset_index(drop = True, inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T02:32:09.039536Z","iopub.execute_input":"2025-02-12T02:32:09.039947Z","iopub.status.idle":"2025-02-12T02:32:09.115811Z","shell.execute_reply.started":"2025-02-12T02:32:09.039915Z","shell.execute_reply":"2025-02-12T02:32:09.114886Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ntest_texts = preprocess_df[\"OriginalTweet\"].tolist()\ntrue_labels = preprocess_df[\"Sentiment\"].tolist()\npredicted_labels = [predict_sentiment(text) for text in test_texts]\n\n\naccuracy = accuracy_score(true_labels, predicted_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T02:32:19.097848Z","iopub.execute_input":"2025-02-12T02:32:19.098200Z","iopub.status.idle":"2025-02-12T02:32:50.840489Z","shell.execute_reply.started":"2025-02-12T02:32:19.098170Z","shell.execute_reply":"2025-02-12T02:32:50.839681Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.9243\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"테스트셋의 정확도가 92.43%가량으로 꽤나 정확한 편. ","metadata":{}},{"cell_type":"markdown","source":"---------------------------------------------------\n### Word2Vec ","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\nimport gensim.downloader as api\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport pandas as pd \nimport numpy as np\nimport nltk\nimport re \nimport os \nimport random \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom io import StringIO\nfrom nltk.corpus import stopwords\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nimport chardet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:06:08.077932Z","iopub.execute_input":"2025-02-12T03:06:08.078523Z","iopub.status.idle":"2025-02-12T03:06:45.014119Z","shell.execute_reply.started":"2025-02-12T03:06:08.078491Z","shell.execute_reply":"2025-02-12T03:06:45.013431Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def convert_sentiment(x):\n    if x == 'Extremely Positive':\n        x = 2\n    elif x == 'Positive':\n        x = 2\n    elif x == 'Neutral':\n        x = 1\n    elif x == 'Negative':\n        x = 0\n    else:\n        x = 0\n    return x\n\n\ndef preprocess_text(x):\n    x = re.sub(r'@\\S*', \"\", x)\n    x = re.sub(r'https?:\\/\\/\\S*', \"\", x)\n    x = re.sub(r'#\\S*', \"\", x)\n    x = re.sub('\\n', \"\", x)\n    x = re.sub('', \"'\", x)\n    x = x.lower()\n    return x.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:08:33.881270Z","iopub.execute_input":"2025-02-12T03:08:33.881620Z","iopub.status.idle":"2025-02-12T03:08:33.886746Z","shell.execute_reply.started":"2025-02-12T03:08:33.881592Z","shell.execute_reply":"2025-02-12T03:08:33.885942Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"with open('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', \n          'r', encoding= 'utf-8', errors='replace') as f:\n    raw_text = f.read()  \n\ndf = pd.read_csv(StringIO(raw_text))\n\nclean_df = df.copy()\nclean_df = clean_df[~clean_df.map(lambda x: isinstance(x, str) and '�' in x).any(axis=1)]\npreprocess_df = clean_df.copy()\npreprocess_df['Sentiment'] = preprocess_df['Sentiment'].apply(convert_sentiment)\npreprocess_df['OriginalTweet'] = preprocess_df['OriginalTweet'].apply(preprocess_text)\npreprocess_df.drop(columns = ['UserName', 'ScreenName','Location', 'TweetAt'], inplace = True)\npreprocess_df = preprocess_df[preprocess_df['OriginalTweet'].str.split().str.len() > 5]\npreprocess_df.reset_index(drop = True, inplace = True)\npreprocess_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:08:36.625850Z","iopub.execute_input":"2025-02-12T03:08:36.626141Z","iopub.status.idle":"2025-02-12T03:08:37.541849Z","shell.execute_reply.started":"2025-02-12T03:08:36.626120Z","shell.execute_reply":"2025-02-12T03:08:37.540956Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                       OriginalTweet  Sentiment\n0  advice talk to your neighbours family to excha...          2\n1  coronavirus australia: woolworths to give elde...          2\n2  my food stock is not the only one which is emp...          2\n3  me, ready to go at supermarket during the  out...          0\n4  as news of the region's first confirmed covid-...          2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>advice talk to your neighbours family to excha...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>coronavirus australia: woolworths to give elde...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>my food stock is not the only one which is emp...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>me, ready to go at supermarket during the  out...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>as news of the region's first confirmed covid-...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(preprocess_df['OriginalTweet'])\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\n\nsequences = tokenizer.texts_to_sequences(preprocess_df['OriginalTweet'])\nMAX_SEQUENCE_LENGTH = max([len(seq) for seq in sequences])\n\npadded_sequences = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\nlabels = np.array(preprocess_df['Sentiment'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:08:41.154596Z","iopub.execute_input":"2025-02-12T03:08:41.155024Z","iopub.status.idle":"2025-02-12T03:08:42.962848Z","shell.execute_reply.started":"2025-02-12T03:08:41.154987Z","shell.execute_reply":"2025-02-12T03:08:42.961924Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 방법 1: LSTM모델에서 임베딩\n\nlstm_model = Sequential([\n    Embedding(input_dim = vocab_size, output_dim = 100, input_length = MAX_SEQUENCE_LENGTH),\n    LSTM(128, return_sequences = True),\n    Dropout(0.5),\n    LSTM(64),\n    Dropout(0.5),\n    Dense(32, activation = 'relu'),\n    Dense(3, activation = 'softmax')\n])\n\nlstm_model.compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer ='adam',\n    metrics = ['accuracy']\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:08:45.492637Z","iopub.execute_input":"2025-02-12T03:08:45.492918Z","iopub.status.idle":"2025-02-12T03:08:46.005765Z","shell.execute_reply.started":"2025-02-12T03:08:45.492896Z","shell.execute_reply":"2025-02-12T03:08:46.004823Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"text_train, text_val, label_train, label_val = train_test_split(padded_sequences, labels,\n                                                               test_size = 0.2, random_state = 42)\n\nhistory = lstm_model.fit(\n    text_train, label_train,\n    validation_data = (text_val, label_val),\n    epochs = 10,\n    batch_size = 32,\n    verbose = 1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:08:50.771400Z","iopub.execute_input":"2025-02-12T03:08:50.771705Z","iopub.status.idle":"2025-02-12T03:10:38.755739Z","shell.execute_reply.started":"2025-02-12T03:08:50.771683Z","shell.execute_reply":"2025-02-12T03:10:38.754826Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.4703 - loss: 0.9876 - val_accuracy: 0.6519 - val_loss: 0.8256\nEpoch 2/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.7050 - loss: 0.7366 - val_accuracy: 0.7695 - val_loss: 0.6211\nEpoch 3/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.8136 - loss: 0.5099 - val_accuracy: 0.8328 - val_loss: 0.4835\nEpoch 4/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.8964 - loss: 0.3249 - val_accuracy: 0.8657 - val_loss: 0.3962\nEpoch 5/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9305 - loss: 0.2291 - val_accuracy: 0.8614 - val_loss: 0.4047\nEpoch 6/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9481 - loss: 0.1692 - val_accuracy: 0.8725 - val_loss: 0.4222\nEpoch 7/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9597 - loss: 0.1300 - val_accuracy: 0.8733 - val_loss: 0.4768\nEpoch 8/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9677 - loss: 0.1022 - val_accuracy: 0.8670 - val_loss: 0.4517\nEpoch 9/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9710 - loss: 0.0937 - val_accuracy: 0.8532 - val_loss: 0.5020\nEpoch 10/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9780 - loss: 0.0710 - val_accuracy: 0.8606 - val_loss: 0.5743\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"LSTM 베이스 모델은 대략 val Accuracy가 86% 정도. 이번엔 임베딩 방식을 사전학습된 word2vec으로 해보자","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n# 1️⃣ `.model` 파일 경로 설정 (이미 /kaggle/working/ 에 복사됨)\nmodel_dest = \"/kaggle/working/word2vec-google-news-300.model\"\n\n# 2️⃣ KeyedVectors로 모델 로드 (Word2Vec.load() 대신 사용!)\nword2vec_model = KeyedVectors.load(model_dest)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:02:55.655850Z","iopub.execute_input":"2025-02-12T04:02:55.656159Z","iopub.status.idle":"2025-02-12T04:03:26.279482Z","shell.execute_reply.started":"2025-02-12T04:02:55.656137Z","shell.execute_reply":"2025-02-12T04:03:26.278702Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# 방법2: word2vec으로 임베딩\n\n\n\n\ntoeknizer = Tokenizer()\ntokenizer.fit_on_texts(preprocess_df['OriginalTweet'])\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\n\n\nembedding_dim = 300\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n\nembedding_layer = Embedding(input_dim = vocab_size, output_dim = embedding_dim,\n                           weights = [embedding_matrix], input_length = MAX_SEQUENCE_LENGTH,\n                           trainable = False)\n\nmodel = Sequential([\n    embedding_layer,\n    LSTM(128, return_sequences = True),\n    Dropout(0.5),\n    LSTM(64),\n    Dropout(0.5),\n    Dense(32, activation = 'relu'),\n    Dense(3, activation = 'softmax')\n])\n\nmodel.compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = ['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:06:35.321649Z","iopub.execute_input":"2025-02-12T04:06:35.321965Z","iopub.status.idle":"2025-02-12T04:06:36.588429Z","shell.execute_reply.started":"2025-02-12T04:06:35.321941Z","shell.execute_reply":"2025-02-12T04:06:36.587490Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(preprocess_df['OriginalTweet'])\nMAX_SEQUENCE_LENGTH = max([len(seq) for seq in sequences])\n\npadded_sequences = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\nlabels = np.array(preprocess_df['Sentiment'])\n\n\ntext_train, text_val, label_train, label_val = train_test_split(padded_sequences, labels,\n                                                               test_size = 0.2, random_state = 42)\n\nhistory = model.fit(\n    text_train, label_train,\n    validation_data = (text_val, label_val),\n    epochs = 10,\n    batch_size = 32,\n    verbose = 1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:06:45.912045Z","iopub.execute_input":"2025-02-12T04:06:45.912333Z","iopub.status.idle":"2025-02-12T04:08:10.631484Z","shell.execute_reply.started":"2025-02-12T04:06:45.912313Z","shell.execute_reply":"2025-02-12T04:08:10.630578Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.4662 - loss: 0.9793 - val_accuracy: 0.6790 - val_loss: 0.7776\nEpoch 2/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.7025 - loss: 0.7426 - val_accuracy: 0.7585 - val_loss: 0.6159\nEpoch 3/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.7896 - loss: 0.5709 - val_accuracy: 0.7981 - val_loss: 0.5469\nEpoch 4/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8161 - loss: 0.5102 - val_accuracy: 0.8008 - val_loss: 0.5155\nEpoch 5/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8268 - loss: 0.4691 - val_accuracy: 0.8253 - val_loss: 0.4890\nEpoch 6/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8506 - loss: 0.4239 - val_accuracy: 0.8232 - val_loss: 0.4871\nEpoch 7/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8592 - loss: 0.3966 - val_accuracy: 0.8361 - val_loss: 0.4437\nEpoch 8/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8785 - loss: 0.3519 - val_accuracy: 0.8419 - val_loss: 0.4543\nEpoch 9/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8911 - loss: 0.3199 - val_accuracy: 0.8385 - val_loss: 0.4642\nEpoch 10/10\n\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9007 - loss: 0.2937 - val_accuracy: 0.8590 - val_loss: 0.4123\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"LSTM 모델에서 임베딩을 했을 땐 과적합되는 경향이 있었으나 (train 데이터 셋 정확도 96%, val셋 86%), pre_trained model에선 비교적 과적합되는 경향이 적다. 다만, val 정확도는 조금 더 낮은 편.","metadata":{}}]}