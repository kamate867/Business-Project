{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9971778,"sourceType":"datasetVersion","datasetId":6134906},{"sourceId":9971800,"sourceType":"datasetVersion","datasetId":6134926},{"sourceId":176725,"sourceType":"modelInstanceVersion","modelInstanceId":150506,"modelId":172986}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nimport os\nfrom collections import defaultdict\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers, optimizers, backend, models\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import display\nimport argparse\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:40:56.178865Z","iopub.execute_input":"2024-11-25T03:40:56.180712Z","iopub.status.idle":"2024-11-25T03:41:07.435886Z","shell.execute_reply.started":"2024-11-25T03:40:56.180670Z","shell.execute_reply":"2024-11-25T03:41:07.435171Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def show_samples(data_path, img_path):\n\tall_imgs = os.listdir(img_path)\n\tsample_img = random.sample(all_imgs, 9)\n\tsample_img_path   =  [  os.path.join(img_path, sample)  for sample in sample_img ]\n\t\n\tfig, axes = plt.subplots(3, 3 , figsize = (10, 10) )\n\tfor ax, img in zip(axes.flatten(), sample_img_path):\n\t\timage = Image.open(img)\n\t\tax.imshow(image)\n\t\tax.axis('off')\n\t\n\tplt.tight_layout()\n\tplt.subplots_adjust(top = 0.9)\n\tplt.suptitle(\"Sample Images\" , fontsize = 16)\n\tplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:52:24.827163Z","iopub.execute_input":"2024-11-24T11:52:24.827805Z","iopub.status.idle":"2024-11-24T11:52:24.833901Z","shell.execute_reply.started":"2024-11-24T11:52:24.827772Z","shell.execute_reply":"2024-11-24T11:52:24.832978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"persimmon_model  = load_model('/kaggle/input/persimmon_bansi_inceptionv3/keras/default/1/cabbage_model.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:52:30.000073Z","iopub.execute_input":"2024-11-24T09:52:30.000428Z","iopub.status.idle":"2024-11-24T09:52:32.823471Z","shell.execute_reply.started":"2024-11-24T09:52:30.000397Z","shell.execute_reply":"2024-11-24T09:52:32.822812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patterns = [ \"11_1TOP\", \"2_2FR45\", \"7_2FR45\", \"12_2FR45\", \"17_2FR45\",\n        \"22_2FR45\", \"27_2FR45\", \"32_2FR45\", \"37_2FR45\", \"21_1TOP\" ]\n\n\ndef select_imgs(img_path):\n    selected_imgs_list = [img for img in img_path if any(pattern in img for pattern in patterns) ]\n    return selected_imgs_list\n\n\ndata_path_2 = '/kaggle/input/test-persimmon-bansi'\n\nimg_path_2s = os.path.join(data_path_2, 'persimmon_bansi_S')\nimg_path_2m = os.path.join(data_path_2, 'persimmon_bansi_M')\nimg_path_2l = os.path.join(data_path_2, 'persimmon_bansi_L')\n\nimg_2s =  os.listdir(img_path_2s)\nimg_2m =  os.listdir(img_path_2m)\nimg_2l =  os.listdir(img_path_2l)\n\nimg_2s_list = select_imgs(img_2s)\nimg_2m_list = select_imgs(img_2m)\nimg_2l_list = select_imgs(img_2l)\n\n\ntest_img_s =  [os.path.join(img_path_2s, img) for img  in img_2s_list if \"76\" not in img]\ntest_img_m =  [os.path.join(img_path_2m, img) for img  in img_2m_list if \"76\" not in img]\ntest_img_l =  [os.path.join(img_path_2l, img) for img  in img_2l_list if \"76\" not in img]\n\n\n\ntest_persimmon_bansi_s = sorted(test_img_s, key = lambda x: ( \\\n                                    int(x.split(\"bansi_S_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_S_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\ntest_persimmon_bansi_m = sorted(test_img_m, key = lambda x: ( \\\n                                    int(x.split(\"bansi_M_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_M_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\ntest_persimmon_bansi_l = sorted(test_img_l, key = lambda x: ( \\\n                                    int(x.split(\"bansi_L_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_L_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:52:35.075786Z","iopub.execute_input":"2024-11-24T09:52:35.076147Z","iopub.status.idle":"2024-11-24T09:52:35.208863Z","shell.execute_reply.started":"2024-11-24T09:52:35.076118Z","shell.execute_reply":"2024-11-24T09:52:35.208228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped_images = defaultdict(list)\n\ntest_img_paths = test_persimmon_bansi_s + test_persimmon_bansi_m + test_persimmon_bansi_l\n\nfor img_path in test_img_paths:\n\tfruit_id = img_path.split(\"-\")[2].split(\"_\")[-1]\n\tgrouped_images[fruit_id].append(img_path)\n\ndef predict_group(model, image_paths):\n\tpreds = []\n\n\tfor img_path in image_paths:\n\t\timg = load_img(img_path, target_size = (224, 224))\n\t\timg_array = img_to_array(img) / 255.0\n\t\timg_array = np.expand_dims(img_array, axis = 0)\n\n\t\tpred = mode.predict(img_array)\n\t\tpreds.append(pred)\n\n\tavg_pred = np.mean(preds, axis = 0)\n\t\n\tfinal_class = np.argmax(avg_pred)\n\n\treturn final_class, avg_pred\n\n\ncorrect_predictions = 0\ntotal_predictions = 0\n\n\nfor fruit_id, img_paths in grouped_images.items():\n\n    if 'S' in fruit_id:\n        true_label = 0\n    elif 'M' in fruit_id:\n        true_label = 1\n    elif 'L' in fruit_id:\n        true_label = 2\n    else:\n        continue\n\n   \n    final_class, avg_pred = predict_group(persimmon_model, img_paths)\n\n\n    if final_class == true_label:\n        correct_predictions += 1\n\n    total_predictions += 1\n\n\naccuracy = correct_predictions / total_predictions\n\nprint(accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**------------------------------------------**","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nimport os\nfrom collections import defaultdict\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers, optimizers, backend, models\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import display\nimport argparse\nimport tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:56:16.174603Z","iopub.execute_input":"2024-11-25T03:56:16.174931Z","iopub.status.idle":"2024-11-25T03:56:16.181118Z","shell.execute_reply.started":"2024-11-25T03:56:16.174905Z","shell.execute_reply":"2024-11-25T03:56:16.179972Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"patterns = [ \"11_1TOP\", \"2_2FR45\", \"7_2FR45\", \"12_2FR45\", \"17_2FR45\",\n        \"22_2FR45\", \"27_2FR45\", \"32_2FR45\", \"37_2FR45\", \"21_1TOP\" ]\n\ndef select_imgs(img_path):\n    selected_imgs_list = [img for img in img_path if any(pattern in img for pattern in patterns) ]\n    return selected_imgs_list\n\n\ndata_path = '/kaggle/input/training-persimmon-bansi'\ndata_path_2 = '/kaggle/input/test-persimmon-bansi'\n\nimg_path_s = os.path.join(data_path, 'persimmon_bansi_S')\nimg_path_m = os.path.join(data_path, 'persimmon_bansi_M')\nimg_path_l = os.path.join(data_path, 'persimmon_bansi_L')\nimg_path_2s = os.path.join(data_path_2, 'persimmon_bansi_S')\nimg_path_2m = os.path.join(data_path_2, 'persimmon_bansi_M')\nimg_path_2l = os.path.join(data_path_2, 'persimmon_bansi_L')\n\nimg_s =  os.listdir(img_path_s)\nimg_m =  os.listdir(img_path_m)\nimg_l =  os.listdir(img_path_l)\nimg_2s =  os.listdir(img_path_2s)\nimg_2m =  os.listdir(img_path_2m)\nimg_2l =  os.listdir(img_path_2l)\n\nimg_s_list = select_imgs(img_s)\nimg_m_list = select_imgs(img_m)\nimg_l_list = select_imgs(img_l)\nimg_2s_list = select_imgs(img_2s)\nimg_2m_list = select_imgs(img_2m)\nimg_2l_list = select_imgs(img_2l)\n\ntraining_img_s =  [os.path.join(img_path_s, img) for img  in img_s_list if '59' not in img]\ntraining_img_m =  [os.path.join(img_path_m, img) for img  in img_m_list if '59' not in img]\ntraining_img_l =  [os.path.join(img_path_l, img) for img  in img_l_list if '59' not in img]\n\ntest_img_s =  [os.path.join(img_path_2s, img) for img  in img_2s_list if \"76\" not in img]\ntest_img_m =  [os.path.join(img_path_2m, img) for img  in img_2m_list if \"76\" not in img]\ntest_img_l =  [os.path.join(img_path_2l, img) for img  in img_2l_list if \"76\" not in img]\n\n\n# persimmon_bansi_training : 1 ~ 58 (delete 59)\n# persimmon_bansi_test : 77 ~ 84 (delete 76)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:56:19.136527Z","iopub.execute_input":"2024-11-25T03:56:19.137159Z","iopub.status.idle":"2024-11-25T03:56:19.174738Z","shell.execute_reply.started":"2024-11-25T03:56:19.137129Z","shell.execute_reply":"2024-11-25T03:56:19.173864Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"training_persimmon_bansi_s = sorted(training_img_s, key = lambda x: ( \\\n                                    int(x.split(\"bansi_S_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_S_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\ntraining_persimmon_bansi_m = sorted(training_img_m, key = lambda x: ( \\\n                                    int(x.split(\"bansi_M_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_M_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\ntraining_persimmon_bansi_l = sorted(training_img_l, key = lambda x: ( \\\n                                    int(x.split(\"bansi_L_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_L_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\n\ntest_persimmon_bansi_s = sorted(test_img_s, key = lambda x: ( \\\n                                    int(x.split(\"bansi_S_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_S_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\ntest_persimmon_bansi_m = sorted(test_img_m, key = lambda x: ( \\\n                                    int(x.split(\"bansi_M_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_M_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )\n\ntest_persimmon_bansi_l = sorted(test_img_l, key = lambda x: ( \\\n                                    int(x.split(\"bansi_L_\")[1].split(\"-\")[0]),\n                                    int(x.split(\"bansi_L_\")[1].split(\"-\")[1].split(\"_\")[0])\n                                   ) )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:56:21.847045Z","iopub.execute_input":"2024-11-25T03:56:21.847963Z","iopub.status.idle":"2024-11-25T03:56:21.859231Z","shell.execute_reply.started":"2024-11-25T03:56:21.847927Z","shell.execute_reply":"2024-11-25T03:56:21.858427Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test_images_dict = defaultdict(list)\ntrain_images_dict = defaultdict(list)\n\ntest_img_paths = test_persimmon_bansi_s + test_persimmon_bansi_m + test_persimmon_bansi_l\ntrain_img_paths = training_persimmon_bansi_s + training_persimmon_bansi_m + training_persimmon_bansi_l\n\n\nfor img_path in test_img_paths:\n\tfruit_id = img_path.split(\"-\")[2].split(\"persimmon_bansi_\")[2]\n\ttest_images_dict[fruit_id].append(img_path)\n\n\nfor img_path in train_img_paths:\n\tfruit_id = img_path.split(\"-\")[2].split(\"persimmon_bansi_\")[2]\n\ttrain_images_dict[fruit_id].append(img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:56:30.982502Z","iopub.execute_input":"2024-11-25T03:56:30.982818Z","iopub.status.idle":"2024-11-25T03:56:30.989448Z","shell.execute_reply.started":"2024-11-25T03:56:30.982794Z","shell.execute_reply":"2024-11-25T03:56:30.988593Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!pip install open3d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:56:39.502194Z","iopub.execute_input":"2024-11-25T03:56:39.503133Z","iopub.status.idle":"2024-11-25T03:56:48.192374Z","shell.execute_reply.started":"2024-11-25T03:56:39.503085Z","shell.execute_reply":"2024-11-25T03:56:48.191207Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: open3d in /opt/conda/lib/python3.10/site-packages (0.18.0)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from open3d) (1.26.4)\nRequirement already satisfied: dash>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from open3d) (2.18.2)\nRequirement already satisfied: werkzeug>=2.2.3 in /opt/conda/lib/python3.10/site-packages (from open3d) (3.0.4)\nRequirement already satisfied: nbformat>=5.7.0 in /opt/conda/lib/python3.10/site-packages (from open3d) (5.10.4)\nRequirement already satisfied: configargparse in /opt/conda/lib/python3.10/site-packages (from open3d) (1.7)\nRequirement already satisfied: ipywidgets>=8.0.4 in /opt/conda/lib/python3.10/site-packages (from open3d) (8.1.5)\nRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from open3d) (2.4.0)\nRequirement already satisfied: pillow>=9.3.0 in /opt/conda/lib/python3.10/site-packages (from open3d) (10.3.0)\nRequirement already satisfied: matplotlib>=3 in /opt/conda/lib/python3.10/site-packages (from open3d) (3.7.5)\nRequirement already satisfied: pandas>=1.0 in /opt/conda/lib/python3.10/site-packages (from open3d) (2.2.2)\nRequirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from open3d) (6.0.2)\nRequirement already satisfied: scikit-learn>=0.21 in /opt/conda/lib/python3.10/site-packages (from open3d) (1.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open3d) (4.66.4)\nRequirement already satisfied: pyquaternion in /opt/conda/lib/python3.10/site-packages (from open3d) (0.9.9)\nRequirement already satisfied: Flask<3.1,>=1.0.4 in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (3.0.3)\nRequirement already satisfied: plotly>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (5.22.0)\nRequirement already satisfied: dash-html-components==2.0.0 in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (2.0.0)\nRequirement already satisfied: dash-core-components==2.0.0 in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (2.0.0)\nRequirement already satisfied: dash-table==5.0.0 in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (5.0.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (7.0.0)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (4.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (2.32.3)\nRequirement already satisfied: retrying in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (1.3.3)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (1.6.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (70.0.0)\nRequirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (8.21.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (5.14.3)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3->open3d) (2.9.0.post0)\nRequirement already satisfied: fastjsonschema>=2.15 in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (2.19.1)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (4.22.0)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (5.7.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->open3d) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->open3d) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (1.8.2)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.18.1)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (3.11.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->dash>=2.6.0->open3d) (2024.8.30)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.2)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"angles_directions = {\n    '11_1TOP': (-90, 'front'),\n    '21_1TOP': (90, 'front'),\n    '2_2FR45': (-45, 'front'),\n    '7_2FR45': (-45, 'right'),\n    '12_2FR45': (-45, 'back'),\n    '17_2FR45': (-45, 'left'),\n    '22_2FR45': (45, 'front'),\n    '27_2FR45': (45, 'right'),\n    '32_2FR45': (45, 'back'),\n    '37_2FR45': (45, 'left'),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:58:34.713592Z","iopub.execute_input":"2024-11-25T03:58:34.713933Z","iopub.status.idle":"2024-11-25T03:58:34.718829Z","shell.execute_reply.started":"2024-11-25T03:58:34.713904Z","shell.execute_reply":"2024-11-25T03:58:34.717958Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def load_multi_view_images(image_paths, target_size=(112, 112)):\n    views = []\n    for img_path in image_paths:\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, target_size)\n        img_array = img / 255.0\n        views.append(img_array)\n    return np.array(views, dtype=np.float32)\n\nall_views = []\nlabels = []\n\nfor group_name, img_paths in train_images_dict.items():\n    if 'S' in group_name:\n        label = 0\n    elif 'M' in group_name:\n        label = 1\n    elif 'L' in group_name:\n        label = 2\n    else:\n        continue\n\n    views = load_multi_view_images(img_paths)\n    all_views.append(views)\n    labels.append(label)\n\nall_views = np.array(all_views, dtype=np.float32)\nlabels = np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:10:07.720262Z","iopub.execute_input":"2024-11-25T05:10:07.721030Z","iopub.status.idle":"2024-11-25T05:10:48.068171Z","shell.execute_reply.started":"2024-11-25T05:10:07.720996Z","shell.execute_reply":"2024-11-25T05:10:48.067195Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Concatenate\nfrom tensorflow.keras.models import Model\n\nX_train, X_val, y_train, y_val = train_test_split(all_views, labels, test_size=0.2, random_state=42)\n\n\ndef build_mvcnn_model(input_shape, num_views, num_classes):\n    inputs = [Input(shape=input_shape) for _ in range(num_views)]\n\n    shared_cnn = tf.keras.Sequential([\n        Conv2D(32, (3, 3), activation='relu'),\n        MaxPooling2D((2, 2)),\n        Flatten()\n    ])\n\n    view_features = [shared_cnn(inp) for inp in inputs]\n    \n\n    merged = Concatenate()(view_features)\n    x = Dense(256, activation='relu')(merged)\n    x = Dense(128, activation='relu')(x)\n    output = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=output)\n    return model\n\n\ninput_shape = (112, 112, 3)\nnum_views = X_train.shape[1] \nnum_classes = 3\n\nmvcnn_model = build_mvcnn_model(input_shape, num_views, num_classes)\nmvcnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\ntrain_inputs = [X_train[:, i] for i in range(num_views)]\nval_inputs = [X_val[:, i] for i in range(num_views)]\n\nearly_stopping = EarlyStopping(\n    monitor = 'val_accuracy',\n    min_delta = 0.05,\n    patience = 5,\n    restore_best_weights = True\n)\n\nhistory = mvcnn_model.fit(train_inputs, y_train, validation_data=(val_inputs, y_val),\\\n                          epochs=20, batch_size=8, verbose=1, callbacks = [early_stopping])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:12:42.454809Z","iopub.execute_input":"2024-11-25T05:12:42.455140Z","iopub.status.idle":"2024-11-25T05:13:07.839971Z","shell.execute_reply.started":"2024-11-25T05:12:42.455113Z","shell.execute_reply":"2024-11-25T05:13:07.839234Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732511565.325544     648 service.cc:145] XLA service 0x7fc7c0003ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1732511565.325598     648 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1732511565.325604     648 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 2/18\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.2500 - loss: 63.5330","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1732511569.924685     648 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 453ms/step - accuracy: 0.3167 - loss: 67.9854 - val_accuracy: 0.2286 - val_loss: 2.5133\nEpoch 2/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5429 - loss: 3.1871 - val_accuracy: 0.9714 - val_loss: 0.1640\nEpoch 3/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.0791 - val_accuracy: 1.0000 - val_loss: 0.0281\nEpoch 4/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9989 - loss: 0.0178 - val_accuracy: 0.9714 - val_loss: 0.1135\nEpoch 5/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0309 - val_accuracy: 1.0000 - val_loss: 0.0131\nEpoch 6/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0040\nEpoch 7/20\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 2.5167e-04 - val_accuracy: 1.0000 - val_loss: 0.0016\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"test_images_dict\ntest_all_views = []\ntest_labels = []\n\nfor group_name, img_paths in test_images_dict.items():\n    if 'S' in group_name:\n        label = 0\n    elif 'M' in group_name:\n        label = 1\n    elif 'L' in group_name:\n        label = 2\n    else:\n        continue\n\n    views = load_multi_view_images(img_paths)\n    test_all_views.append(views)\n    test_labels.append(label)\n\ntest_all_views = np.array(test_all_views, dtype=np.float32)\ntest_labels = np.array(test_labels)\n\nnum_views = test_all_views.shape[1]  \ntest_inputs = [test_all_views[:, i] for i in range(num_views)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:22:45.777522Z","iopub.execute_input":"2024-11-25T05:22:45.777858Z","iopub.status.idle":"2024-11-25T05:22:53.799359Z","shell.execute_reply.started":"2024-11-25T05:22:45.777830Z","shell.execute_reply":"2024-11-25T05:22:53.798586Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"loss, accuracy = mvcnn_model.evaluate(test_inputs, test_labels, verbose = 1 ,batch_size = 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T05:23:20.668750Z","iopub.execute_input":"2024-11-25T05:23:20.669079Z","iopub.status.idle":"2024-11-25T05:23:20.842227Z","shell.execute_reply.started":"2024-11-25T05:23:20.669050Z","shell.execute_reply":"2024-11-25T05:23:20.841593Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.1261\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ**","metadata":{}},{"cell_type":"code","source":"\n\nimages = []\nlabels = []\ntest_images = []\ntest_labels = []\n\ndef load_and_label(image_path, label):\n    for image in image_path:\n        img = load_img(image, target_size = (224, 224))\n        img_array =img_to_array(img) / 225.0\n        images.append(img_array)\n        labels.append(label)\n\n\nload_and_label(training_persimmon_bansi_s , 0)\nload_and_label(training_persimmon_bansi_m, 1)\nload_and_label(training_persimmon_bansi_l, 2)\n\ndef test_load_and_label(image_path, label):\n    for image in image_path:\n        img = load_img(image, target_size = (224, 224))\n        img_array =img_to_array(img) / 225.0\n        test_images.append(img_array)\n        test_labels.append(label)\n\ntest_load_and_label(test_persimmon_bansi_s, 0)\ntest_load_and_label(test_persimmon_bansi_m, 1)\ntest_load_and_label(test_persimmon_bansi_l, 2)\n\nimages = np.array(images)\nlabels = np.array(labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T07:16:01.477328Z","iopub.execute_input":"2024-11-24T07:16:01.478089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_train, X_val, y_train, y_val = train_test_split(images, labels, test_size = 0.2, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:25:00.877090Z","iopub.execute_input":"2024-11-24T05:25:00.877456Z","iopub.status.idle":"2024-11-24T05:25:01.192672Z","shell.execute_reply.started":"2024-11-24T05:25:00.877410Z","shell.execute_reply":"2024-11-24T05:25:01.191599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True\n)\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size = 32)\nval_generator = val_datagen.flow(X_val, y_val, batch_size = 32)\ntest_generator = test_datagen.flow(test_images, test_labels, batch_size = 32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:25:01.194090Z","iopub.execute_input":"2024-11-24T05:25:01.194476Z","iopub.status.idle":"2024-11-24T05:25:01.199906Z","shell.execute_reply.started":"2024-11-24T05:25:01.194435Z","shell.execute_reply":"2024-11-24T05:25:01.198890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nbackend.clear_session()\nmodel_base = InceptionV3(weights = 'imagenet', include_top = False )\n\nInceptionV3_model = model_base.output\npool = GlobalAveragePooling2D()(InceptionV3_model)\ndense_1 = layers.Dense(512, activation = 'relu')(pool)\noutput = layers.Dense(3, activation = 'softmax')(dense_1)\n\n\nmodel_InceptionV3 = Model(inputs = model_base.input, outputs = output)\nmodel_InceptionV3.compile(loss = 'sparse_categorical_crossentropy', \n                          optimizer = optimizers.SGD(learning_rate = 1e-4, momentum = 0.9),\n                          metrics = ['accuracy']\n                         )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:25:01.201056Z","iopub.execute_input":"2024-11-24T05:25:01.201370Z","iopub.status.idle":"2024-11-24T05:25:03.405252Z","shell.execute_reply.started":"2024-11-24T05:25:01.201335Z","shell.execute_reply":"2024-11-24T05:25:03.404270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nhistory = model_InceptionV3.fit(\n    train_generator, \n    epochs = 25,\n    validation_data = val_generator,\n    verbose = 1,\n    callbacks = [EarlyStopping(monitor = 'val_accuracy', patience= 5, restore_best_weights = True)]\n)\n\nloss, accuracy= model_InceptionV3.evaluate(test_generator)\nprint(loss, accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:25:03.407568Z","iopub.execute_input":"2024-11-24T05:25:03.407903Z","iopub.status.idle":"2024-11-24T05:32:20.280682Z","shell.execute_reply.started":"2024-11-24T05:25:03.407875Z","shell.execute_reply":"2024-11-24T05:32:20.279803Z"}},"outputs":[],"execution_count":null}]}